{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Training\n",
    "\n",
    "### Goal:\n",
    "<p>Create a cleaned development dataset you can use to complete the modeling step of your project.</p>\n",
    "\n",
    "### Steps:\n",
    "<ul><li>Create dummy or indicator features for categorical variables</li><li>Standardize the magnitude of numeric features using a scaler</li><li>Split into testing and training datasets</li></ul>\n",
    "Review the following questions and apply them to your dataset:<ul><li>Does my data set have any categorical data, such as Gender or day of the week?</li><li>Do my features have data values that range from 0 - 100 or 0-1 or both and more? Â </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6489 entries, 0 to 6488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   duration_as_adoptable  6489 non-null   float64\n",
      " 1   age                    6489 non-null   object \n",
      " 2   breed_mixed            6489 non-null   bool   \n",
      " 3   breed_primary          6489 non-null   object \n",
      " 4   city                   6489 non-null   object \n",
      " 5   coat                   6489 non-null   object \n",
      " 6   color_primary          6489 non-null   object \n",
      " 7   declawed               6489 non-null   bool   \n",
      " 8   distance               6489 non-null   float64\n",
      " 9   gender                 6489 non-null   object \n",
      " 10  goodwith_cats          6489 non-null   object \n",
      " 11  goodwith_children      6489 non-null   object \n",
      " 12  goodwith_dogs          6489 non-null   object \n",
      " 13  hasimage               6489 non-null   bool   \n",
      " 14  hasvideo               6489 non-null   bool   \n",
      " 15  house_trained          6489 non-null   bool   \n",
      " 16  population             6489 non-null   float64\n",
      " 17  shots_current          6489 non-null   bool   \n",
      " 18  size                   6489 non-null   object \n",
      " 19  spayed_neutered        6489 non-null   bool   \n",
      " 20  special_needs          6489 non-null   bool   \n",
      "dtypes: bool(8), float64(3), object(10)\n",
      "memory usage: 709.9+ KB\n"
     ]
    }
   ],
   "source": [
    "adopted = pd.read_csv('data/cats_trimmed.csv')\n",
    "adopted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies!\n",
    "### After converting bools to ints, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adopted\n",
    "df.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']] = adopted.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']].astype('int64')\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop one of each of the dummy category columns so those features don't double-weight anything\n",
    "df.drop(['age_Senior', 'gender_Male', 'size_Extra Large', 'coat_Hairless', 'breed_primary_American Bobtail', 'color_primary_Tabby (Leopard / Spotted)', 'goodwith_children_False', 'goodwith_dogs_False', 'goodwith_cats_False', 'city_Lacey'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed = imp.fit_transform(df)\n",
    "df = pd.DataFrame(imputed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='duration_as_adoptable')\n",
    "y = df.duration_as_adoptable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed_mixed</th>\n",
       "      <th>declawed</th>\n",
       "      <th>distance</th>\n",
       "      <th>hasimage</th>\n",
       "      <th>hasvideo</th>\n",
       "      <th>house_trained</th>\n",
       "      <th>population</th>\n",
       "      <th>shots_current</th>\n",
       "      <th>spayed_neutered</th>\n",
       "      <th>special_needs</th>\n",
       "      <th>age_Adult</th>\n",
       "      <th>age_Baby</th>\n",
       "      <th>age_Young</th>\n",
       "      <th>breed_primary_Abyssinian</th>\n",
       "      <th>breed_primary_American Shorthair</th>\n",
       "      <th>breed_primary_Balinese</th>\n",
       "      <th>breed_primary_Bengal</th>\n",
       "      <th>breed_primary_Birman</th>\n",
       "      <th>breed_primary_Bombay</th>\n",
       "      <th>breed_primary_British Shorthair</th>\n",
       "      <th>breed_primary_Burmese</th>\n",
       "      <th>breed_primary_Calico</th>\n",
       "      <th>breed_primary_Chartreux</th>\n",
       "      <th>breed_primary_Devon Rex</th>\n",
       "      <th>breed_primary_Dilute Calico</th>\n",
       "      <th>breed_primary_Dilute Tortoiseshell</th>\n",
       "      <th>breed_primary_Domestic Long Hair</th>\n",
       "      <th>breed_primary_Domestic Medium Hair</th>\n",
       "      <th>breed_primary_Domestic Short Hair</th>\n",
       "      <th>breed_primary_Egyptian Mau</th>\n",
       "      <th>breed_primary_Exotic Shorthair</th>\n",
       "      <th>breed_primary_Extra-Toes Cat / Hemingway Polydactyl</th>\n",
       "      <th>breed_primary_Himalayan</th>\n",
       "      <th>breed_primary_Maine Coon</th>\n",
       "      <th>breed_primary_Manx</th>\n",
       "      <th>breed_primary_Munchkin</th>\n",
       "      <th>breed_primary_Norwegian Forest Cat</th>\n",
       "      <th>breed_primary_Persian</th>\n",
       "      <th>breed_primary_Pixiebob</th>\n",
       "      <th>breed_primary_Ragamuffin</th>\n",
       "      <th>breed_primary_Ragdoll</th>\n",
       "      <th>breed_primary_Russian Blue</th>\n",
       "      <th>breed_primary_Scottish Fold</th>\n",
       "      <th>breed_primary_Siamese</th>\n",
       "      <th>breed_primary_Silver</th>\n",
       "      <th>breed_primary_Singapura</th>\n",
       "      <th>breed_primary_Snowshoe</th>\n",
       "      <th>breed_primary_Tabby</th>\n",
       "      <th>breed_primary_Tiger</th>\n",
       "      <th>breed_primary_Tonkinese</th>\n",
       "      <th>breed_primary_Torbie</th>\n",
       "      <th>breed_primary_Tortoiseshell</th>\n",
       "      <th>breed_primary_Turkish Angora</th>\n",
       "      <th>breed_primary_Turkish Van</th>\n",
       "      <th>breed_primary_Tuxedo</th>\n",
       "      <th>city_Auburn</th>\n",
       "      <th>city_Bainbridge Island</th>\n",
       "      <th>city_Battle Ground</th>\n",
       "      <th>city_Bellingham</th>\n",
       "      <th>city_Bothell</th>\n",
       "      <th>city_Bremerton</th>\n",
       "      <th>city_Burlington</th>\n",
       "      <th>city_Chehalis</th>\n",
       "      <th>city_Chewelah</th>\n",
       "      <th>city_Coupeville</th>\n",
       "      <th>city_Des Moines</th>\n",
       "      <th>city_Everett</th>\n",
       "      <th>city_Federal Way</th>\n",
       "      <th>city_Ferndale</th>\n",
       "      <th>city_Friday Harbor</th>\n",
       "      <th>city_Kelso</th>\n",
       "      <th>city_Kennewick</th>\n",
       "      <th>city_Kirkland</th>\n",
       "      <th>city_La Center</th>\n",
       "      <th>city_Langley</th>\n",
       "      <th>city_Long Beach</th>\n",
       "      <th>city_Longview</th>\n",
       "      <th>city_Maple Valley</th>\n",
       "      <th>city_McKenna</th>\n",
       "      <th>city_Oakville</th>\n",
       "      <th>city_Ocean Shores</th>\n",
       "      <th>city_Olympia</th>\n",
       "      <th>city_Othello</th>\n",
       "      <th>city_Pasco</th>\n",
       "      <th>city_Port Angeles</th>\n",
       "      <th>city_Port Townsend</th>\n",
       "      <th>city_Pullman</th>\n",
       "      <th>city_Puyallup</th>\n",
       "      <th>city_Quilcene</th>\n",
       "      <th>city_Quincy</th>\n",
       "      <th>city_Raymond</th>\n",
       "      <th>city_Redmond</th>\n",
       "      <th>city_Republic</th>\n",
       "      <th>city_Roslyn</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Sequim</th>\n",
       "      <th>city_Spokane</th>\n",
       "      <th>city_Spokane Valley</th>\n",
       "      <th>city_Stanwood</th>\n",
       "      <th>city_Steilacoom</th>\n",
       "      <th>city_Sultan</th>\n",
       "      <th>city_Sumner</th>\n",
       "      <th>city_Tacoma</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Washougal</th>\n",
       "      <th>city_West Richland</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yakima</th>\n",
       "      <th>coat_Long</th>\n",
       "      <th>coat_Medium</th>\n",
       "      <th>coat_Short</th>\n",
       "      <th>coat_unknown</th>\n",
       "      <th>color_primary_Black</th>\n",
       "      <th>color_primary_Black &amp; White / Tuxedo</th>\n",
       "      <th>color_primary_Blue Cream</th>\n",
       "      <th>color_primary_Blue Point</th>\n",
       "      <th>color_primary_Brown / Chocolate</th>\n",
       "      <th>color_primary_Buff &amp; White</th>\n",
       "      <th>color_primary_Buff / Tan / Fawn</th>\n",
       "      <th>color_primary_Calico</th>\n",
       "      <th>color_primary_Chocolate Point</th>\n",
       "      <th>color_primary_Cream / Ivory</th>\n",
       "      <th>color_primary_Cream Point</th>\n",
       "      <th>color_primary_Dilute Calico</th>\n",
       "      <th>color_primary_Dilute Tortoiseshell</th>\n",
       "      <th>color_primary_Flame Point</th>\n",
       "      <th>color_primary_Gray &amp; White</th>\n",
       "      <th>color_primary_Gray / Blue / Silver</th>\n",
       "      <th>color_primary_Lilac Point</th>\n",
       "      <th>color_primary_Orange &amp; White</th>\n",
       "      <th>color_primary_Orange / Red</th>\n",
       "      <th>color_primary_Seal Point</th>\n",
       "      <th>color_primary_Smoke</th>\n",
       "      <th>color_primary_Tabby (Brown / Chocolate)</th>\n",
       "      <th>color_primary_Tabby (Buff / Tan / Fawn)</th>\n",
       "      <th>color_primary_Tabby (Gray / Blue / Silver)</th>\n",
       "      <th>color_primary_Tabby (Orange / Red)</th>\n",
       "      <th>color_primary_Tabby (Tiger Striped)</th>\n",
       "      <th>color_primary_Torbie</th>\n",
       "      <th>color_primary_Tortoiseshell</th>\n",
       "      <th>color_primary_White</th>\n",
       "      <th>color_primary_unknown</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>goodwith_cats_True</th>\n",
       "      <th>goodwith_cats_unknown</th>\n",
       "      <th>goodwith_children_True</th>\n",
       "      <th>goodwith_children_unknown</th>\n",
       "      <th>goodwith_dogs_True</th>\n",
       "      <th>goodwith_dogs_unknown</th>\n",
       "      <th>size_Large</th>\n",
       "      <th>size_Medium</th>\n",
       "      <th>size_Small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-1.49233</td>\n",
       "      <td>-5.38903</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.52865</td>\n",
       "      <td>-4.48741</td>\n",
       "      <td>-2.86887</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>-1.76561</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>-1.90570</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.75585</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.42834</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.30815</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>-0.25138</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>1.00801</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>0.23704</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>2.26947</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>5.20489</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>6.93423</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>7.19417</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>2.25044</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>16.41265</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>8.29302</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>16.07980</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>3.72142</td>\n",
       "      <td>2.91799</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>22.31936</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>14.43338</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>23.23252</td>\n",
       "      <td>17.54993</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>5.46809</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>3.12429</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>15.76632</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>6.56832</td>\n",
       "      <td>36.01111</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>4.37632</td>\n",
       "      <td>6.96666</td>\n",
       "      <td>7.22416</td>\n",
       "      <td>7.51164</td>\n",
       "      <td>14.20497</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>12.38951</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>10.62272</td>\n",
       "      <td>5.82126</td>\n",
       "      <td>11.46423</td>\n",
       "      <td>12.86019</td>\n",
       "      <td>11.58393</td>\n",
       "      <td>17.14510</td>\n",
       "      <td>5.49519</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>7.75996</td>\n",
       "      <td>6.45684</td>\n",
       "      <td>2.71930</td>\n",
       "      <td>7.25452</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>14.67310</td>\n",
       "      <td>5.90346</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>24.26745</td>\n",
       "      <td>4.29280</td>\n",
       "      <td>5.31317</td>\n",
       "      <td>7.83545</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>4.17548</td>\n",
       "      <td>18.45336</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>9.50760</td>\n",
       "      <td>6.41376</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>7.58020</td>\n",
       "      <td>9.86500</td>\n",
       "      <td>11.01971</td>\n",
       "      <td>7.04949</td>\n",
       "      <td>3.14445</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>2.20856</td>\n",
       "      <td>4.79931</td>\n",
       "      <td>3.32586</td>\n",
       "      <td>2.64598</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>5.28855</td>\n",
       "      <td>1.87333</td>\n",
       "      <td>2.85976</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>15.47040</td>\n",
       "      <td>5.44137</td>\n",
       "      <td>11.96662</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>5.72686</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>10.81581</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>12.69744</td>\n",
       "      <td>4.03782</td>\n",
       "      <td>4.20717</td>\n",
       "      <td>15.19046</td>\n",
       "      <td>5.27636</td>\n",
       "      <td>6.28938</td>\n",
       "      <td>9.18594</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>2.76206</td>\n",
       "      <td>7.72301</td>\n",
       "      <td>4.00953</td>\n",
       "      <td>4.73561</td>\n",
       "      <td>9.94137</td>\n",
       "      <td>8.03402</td>\n",
       "      <td>5.88675</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>4.16299</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>2.03515</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>3.39515</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>2.57568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       breed_mixed   declawed   distance   hasimage   hasvideo  house_trained  population  shots_current  spayed_neutered  special_needs  age_Adult   age_Baby  age_Young  breed_primary_Abyssinian  breed_primary_American Shorthair  breed_primary_Balinese  breed_primary_Bengal  breed_primary_Birman  breed_primary_Bombay  breed_primary_British Shorthair  breed_primary_Burmese  breed_primary_Calico  breed_primary_Chartreux  breed_primary_Devon Rex  breed_primary_Dilute Calico  breed_primary_Dilute Tortoiseshell  breed_primary_Domestic Long Hair  breed_primary_Domestic Medium Hair  breed_primary_Domestic Short Hair  breed_primary_Egyptian Mau  breed_primary_Exotic Shorthair  breed_primary_Extra-Toes Cat / Hemingway Polydactyl  breed_primary_Himalayan  breed_primary_Maine Coon  breed_primary_Manx  breed_primary_Munchkin  breed_primary_Norwegian Forest Cat  breed_primary_Persian  breed_primary_Pixiebob  breed_primary_Ragamuffin  breed_primary_Ragdoll  breed_primary_Russian Blue  breed_primary_Scottish Fold  breed_primary_Siamese  breed_primary_Silver  breed_primary_Singapura  breed_primary_Snowshoe  breed_primary_Tabby  breed_primary_Tiger  breed_primary_Tonkinese  breed_primary_Torbie  breed_primary_Tortoiseshell  breed_primary_Turkish Angora  breed_primary_Turkish Van  breed_primary_Tuxedo  city_Auburn  city_Bainbridge Island  city_Battle Ground  city_Bellingham  city_Bothell  city_Bremerton  city_Burlington  city_Chehalis  city_Chewelah  city_Coupeville  city_Des Moines  city_Everett  city_Federal Way  city_Ferndale  city_Friday Harbor  city_Kelso  city_Kennewick  city_Kirkland  city_La Center  city_Langley  city_Long Beach  city_Longview  city_Maple Valley  city_McKenna  city_Oakville  city_Ocean Shores  city_Olympia  city_Othello  city_Pasco  city_Port Angeles  city_Port Townsend  city_Pullman  city_Puyallup  city_Quilcene  city_Quincy  city_Raymond  city_Redmond  city_Republic  city_Roslyn  city_Seattle  city_Sequim  city_Spokane  city_Spokane Valley  city_Stanwood  city_Steilacoom  city_Sultan  city_Sumner  city_Tacoma  city_Vancouver  city_Washougal  city_West Richland  city_Woodinville  city_Yakima  coat_Long  coat_Medium  coat_Short  coat_unknown  color_primary_Black  color_primary_Black & White / Tuxedo  color_primary_Blue Cream  color_primary_Blue Point  color_primary_Brown / Chocolate  color_primary_Buff & White  color_primary_Buff / Tan / Fawn  color_primary_Calico  color_primary_Chocolate Point  color_primary_Cream / Ivory  color_primary_Cream Point  color_primary_Dilute Calico  color_primary_Dilute Tortoiseshell  color_primary_Flame Point  color_primary_Gray & White  color_primary_Gray / Blue / Silver  color_primary_Lilac Point  color_primary_Orange & White  color_primary_Orange / Red  color_primary_Seal Point  color_primary_Smoke  color_primary_Tabby (Brown / Chocolate)  color_primary_Tabby (Buff / Tan / Fawn)  color_primary_Tabby (Gray / Blue / Silver)  color_primary_Tabby (Orange / Red)  color_primary_Tabby (Tiger Striped)  color_primary_Torbie  color_primary_Tortoiseshell  color_primary_White  color_primary_unknown  gender_Female  goodwith_cats_True  goodwith_cats_unknown  goodwith_children_True  goodwith_children_unknown  goodwith_dogs_True  goodwith_dogs_unknown  size_Large  size_Medium  size_Small\n",
       "count   6489.00000 6489.00000 6489.00000 6489.00000 6489.00000     6489.00000  6489.00000     6489.00000       6489.00000     6489.00000 6489.00000 6489.00000 6489.00000                6489.00000                        6489.00000              6489.00000            6489.00000            6489.00000            6489.00000                       6489.00000             6489.00000            6489.00000               6489.00000               6489.00000                   6489.00000                          6489.00000                        6489.00000                          6489.00000                         6489.00000                  6489.00000                      6489.00000                                           6489.00000               6489.00000                6489.00000          6489.00000              6489.00000                          6489.00000             6489.00000              6489.00000                6489.00000             6489.00000                  6489.00000                   6489.00000             6489.00000            6489.00000               6489.00000              6489.00000           6489.00000           6489.00000               6489.00000            6489.00000                   6489.00000                    6489.00000                 6489.00000            6489.00000   6489.00000              6489.00000          6489.00000       6489.00000    6489.00000      6489.00000       6489.00000     6489.00000     6489.00000       6489.00000       6489.00000    6489.00000        6489.00000     6489.00000          6489.00000  6489.00000      6489.00000     6489.00000      6489.00000    6489.00000       6489.00000     6489.00000         6489.00000    6489.00000     6489.00000         6489.00000    6489.00000    6489.00000  6489.00000         6489.00000          6489.00000    6489.00000     6489.00000     6489.00000   6489.00000    6489.00000    6489.00000     6489.00000   6489.00000    6489.00000   6489.00000    6489.00000           6489.00000     6489.00000       6489.00000   6489.00000   6489.00000   6489.00000      6489.00000      6489.00000          6489.00000        6489.00000   6489.00000 6489.00000   6489.00000  6489.00000    6489.00000           6489.00000                            6489.00000                6489.00000                6489.00000                       6489.00000                  6489.00000                       6489.00000            6489.00000                     6489.00000                   6489.00000                 6489.00000                   6489.00000                          6489.00000                 6489.00000                  6489.00000                          6489.00000                 6489.00000                    6489.00000                  6489.00000                6489.00000           6489.00000                               6489.00000                               6489.00000                                  6489.00000                          6489.00000                           6489.00000            6489.00000                   6489.00000           6489.00000             6489.00000     6489.00000          6489.00000             6489.00000              6489.00000                 6489.00000          6489.00000             6489.00000  6489.00000   6489.00000  6489.00000\n",
       "mean      -0.00000    0.00000   -0.00000   -0.00000   -0.00000        0.00000    -0.00000        0.00000         -0.00000       -0.00000    0.00000    0.00000    0.00000                  -0.00000                           0.00000                -0.00000               0.00000              -0.00000               0.00000                         -0.00000                0.00000               0.00000                 -0.00000                 -0.00000                     -0.00000                            -0.00000                          -0.00000                            -0.00000                            0.00000                     0.00000                        -0.00000                                              0.00000                  0.00000                  -0.00000            -0.00000                 0.00000                            -0.00000               -0.00000                -0.00000                  -0.00000               -0.00000                    -0.00000                     -0.00000               -0.00000               0.00000                  0.00000                 0.00000             -0.00000             -0.00000                 -0.00000              -0.00000                     -0.00000                      -0.00000                   -0.00000              -0.00000     -0.00000                -0.00000             0.00000          0.00000       0.00000         0.00000         -0.00000       -0.00000       -0.00000         -0.00000         -0.00000       0.00000           0.00000       -0.00000            -0.00000    -0.00000         0.00000       -0.00000        -0.00000      -0.00000          0.00000       -0.00000           -0.00000       0.00000        0.00000           -0.00000      -0.00000      -0.00000     0.00000           -0.00000            -0.00000       0.00000       -0.00000        0.00000      0.00000      -0.00000      -0.00000        0.00000      0.00000       0.00000      0.00000       0.00000             -0.00000        0.00000         -0.00000     -0.00000     -0.00000     -0.00000         0.00000         0.00000             0.00000          -0.00000      0.00000    0.00000     -0.00000    -0.00000       0.00000              0.00000                               0.00000                  -0.00000                   0.00000                          0.00000                    -0.00000                         -0.00000               0.00000                       -0.00000                      0.00000                    0.00000                      0.00000                            -0.00000                   -0.00000                    -0.00000                            -0.00000                    0.00000                      -0.00000                    -0.00000                   0.00000             -0.00000                                 -0.00000                                 -0.00000                                    -0.00000                             0.00000                              0.00000               0.00000                      0.00000              0.00000                0.00000        0.00000             0.00000               -0.00000                -0.00000                   -0.00000            -0.00000                0.00000    -0.00000      0.00000     0.00000\n",
       "std        1.00008    1.00008    1.00008    1.00008    1.00008        1.00008     1.00008        1.00008          1.00008        1.00008    1.00008    1.00008    1.00008                   1.00008                           1.00008                 1.00008               1.00008               1.00008               1.00008                          1.00008                1.00008               1.00008                  1.00008                  1.00008                      1.00008                             1.00008                           1.00008                             1.00008                            1.00008                     1.00008                         1.00008                                              1.00008                  1.00008                   1.00008             1.00008                 1.00008                             1.00008                1.00008                 1.00008                   1.00008                1.00008                     1.00008                      1.00008                1.00008               1.00008                  1.00008                 1.00008              1.00008              1.00008                  1.00008               1.00008                      1.00008                       1.00008                    1.00008               1.00008      1.00008                 1.00008             1.00008          1.00008       1.00008         1.00008          1.00008        1.00008        1.00008          1.00008          1.00008       1.00008           1.00008        1.00008             1.00008     1.00008         1.00008        1.00008         1.00008       1.00008          1.00008        1.00008            1.00008       1.00008        1.00008            1.00008       1.00008       1.00008     1.00008            1.00008             1.00008       1.00008        1.00008        1.00008      1.00008       1.00008       1.00008        1.00008      1.00008       1.00008      1.00008       1.00008              1.00008        1.00008          1.00008      1.00008      1.00008      1.00008         1.00008         1.00008             1.00008           1.00008      1.00008    1.00008      1.00008     1.00008       1.00008              1.00008                               1.00008                   1.00008                   1.00008                          1.00008                     1.00008                          1.00008               1.00008                        1.00008                      1.00008                    1.00008                      1.00008                             1.00008                    1.00008                     1.00008                             1.00008                    1.00008                       1.00008                     1.00008                   1.00008              1.00008                                  1.00008                                  1.00008                                     1.00008                             1.00008                              1.00008               1.00008                      1.00008              1.00008                1.00008        1.00008             1.00008                1.00008                 1.00008                    1.00008             1.00008                1.00008     1.00008      1.00008     1.00008\n",
       "min       -1.52233   -0.09497   -1.49233   -5.38903   -0.19213       -1.26894    -0.52865       -4.48741         -2.86887       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793    -1.76561      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454     -1.90570    -0.38825\n",
       "25%       -1.52233   -0.09497   -0.75585    0.18556   -0.19213       -1.26894    -0.42834        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454      0.52474    -0.38825\n",
       "50%        0.65689   -0.09497   -0.30815    0.18556   -0.19213        0.78806    -0.25138        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730            -0.98213               -0.92987                -0.60340                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "75%        0.65689   -0.09497    1.00801    0.18556   -0.19213        0.78806     0.23704        0.22285          0.34857       -0.13900    1.40108    1.09020   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730             1.01820                1.07542                 1.65727                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "max        0.65689   10.52992    2.26947    0.18556    5.20489        0.78806     6.93423        0.22285          0.34857        7.19417    1.40108    1.09020    2.25044                  56.95173                          16.41265                56.95173              40.26475              46.49731              28.46269                         56.95173               80.54812               8.29302                 56.95173                 40.26475                     16.07980                            20.11374                           3.72142                             2.91799                            0.80174                    80.54812                        40.26475                                             22.31936                 28.46269                  14.43338            21.50581                80.54812                            56.95173               14.92510                46.49731                  56.95173               23.23252                    17.54993                     32.87096                5.46809              80.54812                 80.54812                17.98472              3.12429             46.49731                 56.95173              15.76632                      9.64590                      56.95173                   56.95173               6.56832     36.01111                32.87096            32.87096          4.37632       6.96666         7.22416          7.51164       14.20497       46.49731         12.38951         20.11374      10.62272           5.82126       11.46423            12.86019    11.58393        17.14510        5.49519        40.26475       7.75996          6.45684        2.71930            7.25452      21.50581       14.67310            5.90346      14.92510      24.26745     4.29280            5.31317             7.83545      30.43025        4.17548       18.45336     80.54812      56.95173       9.50760        6.41376     28.46269       7.58020      9.86500      11.01971              7.04949        3.14445         32.87096     14.92510     32.87096     30.43025        80.54812        80.54812            28.46269           2.20856      4.79931    3.32586      2.64598     0.56638       5.28855              1.87333                               2.85976                  17.98472                  15.47040                          5.44137                    11.96662                          9.64590               5.72686                       17.98472                     10.52992                   19.51169                     10.81581                             9.64590                   12.69744                     4.03782                             4.20717                   15.19046                       5.27636                     6.28938                   9.18594             19.51169                                  2.76206                                  7.72301                                     4.00953                             4.73561                              9.94137               8.03402                      5.88675              9.64590                4.16299        0.95730             1.01820                1.07542                 1.65727                    0.73053             2.03515                0.58441     3.39515      0.52474     2.57568"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X = pd.DataFrame(scaled, columns=X.columns)\n",
    "scaled_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.3, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542, 152), (1947, 152))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542,), (1947,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_test.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_test.csv\"\n"
     ]
    }
   ],
   "source": [
    "# save training and test sets\n",
    "datapath = 'data/tt_sets'\n",
    "save_file(X_train, 'cats_X_train.csv', datapath)\n",
    "save_file(X_test, 'cats_X_test.csv', datapath)\n",
    "save_file(y_train, 'cats_y_train.csv', datapath)\n",
    "save_file(y_test, 'cats_y_test.csv', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "### Goal: Build two to three different models and identify the best one.\n",
    "<ul><li>Fit your models with a training dataset</li>\n",
    "<li>Review model outcomes â Iterate over additional models as needed</li>\n",
    "<li>Identify the final model that you think is the best model for this project</li></ul>\n",
    " Review the following questions and apply them to your analysis: \n",
    "<ul><li>Does my data involve a time series or forecasting? If so, am I splitting the train and test data appropriately?</li>\n",
    "<li>Is my response variable continuous or categorical?</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7834377487297608"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11047321, 0.06845203, 0.12230434, 0.0906218 , 0.14807579])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_scores_preopt = rf_cv['test_score']\n",
    "rf_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10798543354812433, 0.027139796345526775)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_cv_scores_preopt), np.std(rf_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  34.484918\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)\n",
    "rmse_rf_preopt = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "print(\"RMSE : % f\" %(rmse_rf_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "rf_grid_params = {\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3,4,5, 6,7,8,9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'n_estimators': 483}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gcv = GridSearchCV(rf, param_grid=rf_grid_params, cv=5, n_jobs=-1)\n",
    "gcv.fit(X_train, y_train)\n",
    "gcv_params = gcv.best_params_\n",
    "\n",
    "gcv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41276733178513225"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=gcv_params['n_estimators'], max_depth=gcv_params['max_depth'])\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16132885, 0.15848212, 0.17824084, 0.16251256, 0.18701481]),\n",
       " array([0.10848312, 0.12249882, 0.03004946, 0.07426636, 0.16343863]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_train = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_test = cross_validate(rf, X_test, y_test, cv=5)\n",
    "rf_cv_train['test_score'], rf_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.169515833380018\n",
      "Average CV Score, Trest Set: 0.09974727908856088\n"
     ]
    }
   ],
   "source": [
    "rf_train_score = np.mean(rf_cv_train['test_score'])\n",
    "rf_test_score = np.mean(rf_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", rf_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", rf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  27.639140\n",
      "RMSE Test Set :  32.258146\n"
     ]
    }
   ],
   "source": [
    "rf_train_pred = rf.predict(X_train)\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "rf_rmse_train = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_rmse_test = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(rf_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(rf_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31787013710575396"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18879816, 0.18885649, 0.17443963, 0.18527858, 0.18758045])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_scores_preopt = gb_cv['test_score']\n",
    "gb_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18499066139069795, 0.0054324192768760516)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gb_cv_scores_preopt), np.std(gb_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  32.233148\n"
     ]
    }
   ],
   "source": [
    "gb_pred = gb.predict(X_test)\n",
    "rmse_gb_preopt = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "print(\"RMSE : % f\" %(rmse_gb_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "gb_grid_params = {\n",
    "        'learning_rate': [.01, .1, 1],\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n",
       "                         'n_estimators': [10, 12, 16, 20, 26, 33, 42, 54, 69,\n",
       "                                          88, 112, 143, 183, 233, 297, 379, 483,\n",
       "                                          615, 784, 1000]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_cv = GridSearchCV(gb, param_grid=gb_grid_params, cv=5, n_jobs=-1)\n",
    "gb_grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 297}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_cv.fit(X_train, y_train)\n",
    "gb_grid_cv_params = gb_grid_cv.best_params_\n",
    "\n",
    "gb_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31766102768246784"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=gb_grid_cv_params['n_estimators'], max_depth=gb_grid_cv_params['max_depth'], learning_rate=gb_grid_cv_params['learning_rate'])\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.19986593, 0.19291014, 0.18942762, 0.20295757, 0.20409606]),\n",
       " array([0.18935148, 0.12870794, 0.0390777 , 0.08299523, 0.17471592]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv_train = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_test = cross_validate(gb, X_test, y_test, cv=5)\n",
    "gb_cv_train['test_score'], gb_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.19785146651482036\n",
      "Average CV Score, Trest Set: 0.12296965192450933\n"
     ]
    }
   ],
   "source": [
    "gb_train_score = np.mean(gb_cv_train['test_score'])\n",
    "gb_test_score = np.mean(gb_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", gb_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", gb_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  29.793362\n",
      "RMSE Test Set :  32.136544\n"
     ]
    }
   ],
   "source": [
    "gb_train_pred = gb.predict(X_train)\n",
    "gb_test_pred = gb.predict(X_test)\n",
    "gb_rmse_train = np.sqrt(mean_squared_error(y_train, gb_train_pred))\n",
    "gb_rmse_test = np.sqrt(mean_squared_error(y_test, gb_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(gb_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(gb_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8760428716103522"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=25, weights='distance')\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08815071889321771, 0.018072258030583203)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_cv = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_scores_preopt = kn_cv['test_score']\n",
    "np.mean(kn_cv_scores_preopt), np.std(kn_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  34.766549\n"
     ]
    }
   ],
   "source": [
    "kn_pred = kn.predict(X_test)\n",
    "rmse_kn_preopt = np.sqrt(mean_squared_error(y_test, kn_pred))\n",
    "print(\"RMSE : % f\" %(rmse_kn_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "kn_grid_params = {\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'n_neighbors': n_est,\n",
    "        'p': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 12, 'p': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_grid_cv = GridSearchCV(kn, param_grid=kn_grid_params, cv=5, n_jobs=-1)\n",
    "kn_grid_cv.fit(X_train, y_train)\n",
    "kn_grid_cv_params = kn_grid_cv.best_params_\n",
    "\n",
    "kn_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2893853844980272"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=kn_grid_cv_params['n_neighbors'], weights=kn_grid_cv_params['weights'], p=kn_grid_cv_params['p'])\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.12270817, 0.15985082, 0.15388799, 0.12599193, 0.15656884]),\n",
       " array([ 0.05678765, -0.00627876,  0.06919624,  0.00615119,  0.10881523]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_cv_train = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_test = cross_validate(kn, X_test, y_test, cv=5)\n",
    "kn_cv_train['test_score'], kn_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.14380155080470985\n",
      "Average CV Score, Trest Set: 0.04693430907128952\n"
     ]
    }
   ],
   "source": [
    "kn_train_score = np.mean(kn_cv_train['test_score'])\n",
    "kn_test_score = np.mean(kn_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", kn_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", kn_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  30.404404\n",
      "RMSE Test Set :  33.158672\n"
     ]
    }
   ],
   "source": [
    "kn_train_pred = kn.predict(X_train)\n",
    "kn_test_pred = kn.predict(X_test)\n",
    "kn_rmse_train = np.sqrt(mean_squared_error(y_train, kn_train_pred))\n",
    "kn_rmse_test = np.sqrt(mean_squared_error(y_test, kn_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(kn_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(kn_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5458485737696644"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 50)\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1560183163148017, 0.02676679102213811)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_cv = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_scores_preopt = xg_cv['test_score']\n",
    "np.mean(xg_cv_scores_preopt), np.std(xg_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  32.830385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xg_pred = xg.predict(X_test)\n",
    "rmse_xg_preopt = np.sqrt(mean_squared_error(y_test, xg_pred))\n",
    "print(\"RMSE : % f\" %(rmse_xg_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "xg_grid_params = {\n",
    "        'objective': ['reg:squarederror', 'reg:squaredlogerror', 'reg:logistic'],\n",
    "        'n_estimators': n_est,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20, 'objective': 'reg:squarederror'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_grid_cv = GridSearchCV(xg, param_grid=xg_grid_params, cv=5, n_jobs=-1)\n",
    "xg_grid_cv.fit(X_train, y_train)\n",
    "xg_grid_cv_params = xg_grid_cv.best_params_\n",
    "\n",
    "xg_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4250868532823179"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = xgb.XGBRegressor(objective=xg_grid_cv_params['objective'], n_estimators = xg_grid_cv_params['n_estimators'])\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.17946805, 0.16315177, 0.20408199, 0.15520235, 0.17521979]),\n",
       " array([0.15223908, 0.12042808, 0.00786487, 0.05927666, 0.17337777]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_cv_train = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_test = cross_validate(xg, X_test, y_test, cv=5)\n",
    "xg_cv_train['test_score'], xg_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.17542478958428948\n",
      "Average CV Score, Trest Set: 0.10263729281136572\n"
     ]
    }
   ],
   "source": [
    "xg_train_score = np.mean(xg_cv_train['test_score'])\n",
    "xg_test_score = np.mean(xg_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", xg_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", xg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  27.347684\n",
      "RMSE Test Set :  32.374172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xg_train_pred = xg.predict(X_train)\n",
    "xg_test_pred = xg.predict(X_test)\n",
    "xg_rmse_train = np.sqrt(mean_squared_error(y_train, xg_train_pred))\n",
    "xg_rmse_test = np.sqrt(mean_squared_error(y_test, xg_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(xg_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(xg_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Train</th>\n",
       "      <th>RMSE Train</th>\n",
       "      <th>CV Test</th>\n",
       "      <th>RMSE Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.16952</td>\n",
       "      <td>27.63914</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>32.25815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>0.19785</td>\n",
       "      <td>29.79336</td>\n",
       "      <td>0.12297</td>\n",
       "      <td>32.13654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.14380</td>\n",
       "      <td>30.40440</td>\n",
       "      <td>0.04693</td>\n",
       "      <td>33.15867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.17542</td>\n",
       "      <td>27.34768</td>\n",
       "      <td>0.10264</td>\n",
       "      <td>32.37417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CV Train  RMSE Train  CV Test  RMSE Test\n",
       "RandomForest       0.16952    27.63914  0.09975   32.25815\n",
       "GradientBoosting   0.19785    29.79336  0.12297   32.13654\n",
       "KNNeighbors        0.14380    30.40440  0.04693   33.15867\n",
       "XGBoost            0.17542    27.34768  0.10264   32.37417"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame({'CV Train': [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score)], 'RMSE Train': [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train],'CV Test': [np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)], 'RMSE Test': [rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]}, index=['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost'])\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best CV Score: \n",
      "Train: KNNeighbors \n",
      "Test: KNNeighbors\n",
      "\n",
      "Model with best RMSE: \n",
      "Train: XGBoost \n",
      "Test: GradientBoosting\n"
     ]
    }
   ],
   "source": [
    "print(\"Model with best CV Score: \\nTrain:\", model_scores['CV Train'].idxmin(), \"\\nTest:\", model_scores['CV Test'].idxmin())\n",
    "print(\"\\nModel with best RMSE: \\nTrain:\", model_scores['RMSE Train'].idxmin(), \"\\nTest:\", model_scores['RMSE Test'].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost','RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost']\n",
    "cv_scores_all = [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score), np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)]\n",
    "types = ['train', 'train', 'train', 'train', 'test', 'test', 'test', 'test']\n",
    "rmse_scores_all = [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train, rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]\n",
    "\n",
    "cv_scores = pd.DataFrame(list(zip(models, cv_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) \n",
    "rmse_scores = pd.DataFrame(list(zip(models, rmse_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=cv_scores)\n",
    "plt.title(\"CV Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=rmse_scores)\n",
    "plt.title(\"RMSE Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go forward with **GradientBoosting** for the cats data. I'm choosing this model because it has the best RMSE score for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.490261099806496"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_test = np.std(y_test)\n",
    "std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_scores_test = model_scores.drop(columns=['CV Train','RMSE Train']).rename(columns={'CV Test':'R^2', 'RMSE Test':'RMSE'})\n",
    "\n",
    "#standardize the RMSE by dividing it by the standard deviation of y_test\n",
    "model_scores_test['RMSE'] = model_scores_test['RMSE']/std_test\n",
    "\n",
    "#sort scores by lowest RMSE\n",
    "model_scores_test.sort_values(by='RMSE', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=np.round(model_scores_test.values,4), \n",
    "    colLabels=model_scores_test.columns, \n",
    "    rowLabels=model_scores_test.index, \n",
    "    loc='center', \n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.scale(.5, 2.5)\n",
    "\n",
    "plt.title('Model Metrics: Cats')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('assets/cats_model_metrics.png', bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
