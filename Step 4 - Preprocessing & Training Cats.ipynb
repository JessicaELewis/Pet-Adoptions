{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Training\n",
    "\n",
    "### Goal:\n",
    "<p>Create a cleaned development dataset you can use to complete the modeling step of your project.</p>\n",
    "\n",
    "### Steps:\n",
    "<ul><li>Create dummy or indicator features for categorical variables</li><li>Standardize the magnitude of numeric features using a scaler</li><li>Split into testing and training datasets</li></ul>\n",
    "Review the following questions and apply them to your dataset:<ul><li>Does my data set have any categorical data, such as Gender or day of the week?</li><li>Do my features have data values that range from 0 - 100 or 0-1 or both and more? Â </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6489 entries, 0 to 6488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   duration_as_adoptable  6489 non-null   float64\n",
      " 1   age                    6489 non-null   object \n",
      " 2   breed_mixed            6489 non-null   bool   \n",
      " 3   breed_primary          6489 non-null   object \n",
      " 4   city                   6489 non-null   object \n",
      " 5   coat                   6489 non-null   object \n",
      " 6   color_primary          6489 non-null   object \n",
      " 7   declawed               6489 non-null   bool   \n",
      " 8   distance               6489 non-null   float64\n",
      " 9   gender                 6489 non-null   object \n",
      " 10  goodwith_cats          6489 non-null   object \n",
      " 11  goodwith_children      6489 non-null   object \n",
      " 12  goodwith_dogs          6489 non-null   object \n",
      " 13  hasimage               6489 non-null   bool   \n",
      " 14  hasvideo               6489 non-null   bool   \n",
      " 15  house_trained          6489 non-null   bool   \n",
      " 16  population             6489 non-null   float64\n",
      " 17  shots_current          6489 non-null   bool   \n",
      " 18  size                   6489 non-null   object \n",
      " 19  spayed_neutered        6489 non-null   bool   \n",
      " 20  special_needs          6489 non-null   bool   \n",
      "dtypes: bool(8), float64(3), object(10)\n",
      "memory usage: 709.9+ KB\n"
     ]
    }
   ],
   "source": [
    "adopted = pd.read_csv('data/cats_trimmed.csv')\n",
    "adopted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies!\n",
    "### After converting bools to ints, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adopted\n",
    "df.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']] = adopted.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']].astype('int64')\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop one of each of the dummy category columns so those features don't double-weight anything\n",
    "df.drop(['age_Senior', 'gender_Male', 'size_Extra Large', 'coat_Hairless', 'breed_primary_American Bobtail', 'color_primary_Tabby (Leopard / Spotted)', 'goodwith_children_False', 'goodwith_dogs_False', 'goodwith_cats_False', 'city_Lacey'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed = imp.fit_transform(df)\n",
    "df = pd.DataFrame(imputed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='duration_as_adoptable')\n",
    "y = df.duration_as_adoptable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed_mixed</th>\n",
       "      <th>declawed</th>\n",
       "      <th>distance</th>\n",
       "      <th>hasimage</th>\n",
       "      <th>hasvideo</th>\n",
       "      <th>house_trained</th>\n",
       "      <th>population</th>\n",
       "      <th>shots_current</th>\n",
       "      <th>spayed_neutered</th>\n",
       "      <th>special_needs</th>\n",
       "      <th>age_Adult</th>\n",
       "      <th>age_Baby</th>\n",
       "      <th>age_Young</th>\n",
       "      <th>breed_primary_Abyssinian</th>\n",
       "      <th>breed_primary_American Shorthair</th>\n",
       "      <th>breed_primary_Balinese</th>\n",
       "      <th>breed_primary_Bengal</th>\n",
       "      <th>breed_primary_Birman</th>\n",
       "      <th>breed_primary_Bombay</th>\n",
       "      <th>breed_primary_British Shorthair</th>\n",
       "      <th>breed_primary_Burmese</th>\n",
       "      <th>breed_primary_Calico</th>\n",
       "      <th>breed_primary_Chartreux</th>\n",
       "      <th>breed_primary_Devon Rex</th>\n",
       "      <th>breed_primary_Dilute Calico</th>\n",
       "      <th>breed_primary_Dilute Tortoiseshell</th>\n",
       "      <th>breed_primary_Domestic Long Hair</th>\n",
       "      <th>breed_primary_Domestic Medium Hair</th>\n",
       "      <th>breed_primary_Domestic Short Hair</th>\n",
       "      <th>breed_primary_Egyptian Mau</th>\n",
       "      <th>breed_primary_Exotic Shorthair</th>\n",
       "      <th>breed_primary_Extra-Toes Cat / Hemingway Polydactyl</th>\n",
       "      <th>breed_primary_Himalayan</th>\n",
       "      <th>breed_primary_Maine Coon</th>\n",
       "      <th>breed_primary_Manx</th>\n",
       "      <th>breed_primary_Munchkin</th>\n",
       "      <th>breed_primary_Norwegian Forest Cat</th>\n",
       "      <th>breed_primary_Persian</th>\n",
       "      <th>breed_primary_Pixiebob</th>\n",
       "      <th>breed_primary_Ragamuffin</th>\n",
       "      <th>breed_primary_Ragdoll</th>\n",
       "      <th>breed_primary_Russian Blue</th>\n",
       "      <th>breed_primary_Scottish Fold</th>\n",
       "      <th>breed_primary_Siamese</th>\n",
       "      <th>breed_primary_Silver</th>\n",
       "      <th>breed_primary_Singapura</th>\n",
       "      <th>breed_primary_Snowshoe</th>\n",
       "      <th>breed_primary_Tabby</th>\n",
       "      <th>breed_primary_Tiger</th>\n",
       "      <th>breed_primary_Tonkinese</th>\n",
       "      <th>breed_primary_Torbie</th>\n",
       "      <th>breed_primary_Tortoiseshell</th>\n",
       "      <th>breed_primary_Turkish Angora</th>\n",
       "      <th>breed_primary_Turkish Van</th>\n",
       "      <th>breed_primary_Tuxedo</th>\n",
       "      <th>city_Auburn</th>\n",
       "      <th>city_Bainbridge Island</th>\n",
       "      <th>city_Battle Ground</th>\n",
       "      <th>city_Bellingham</th>\n",
       "      <th>city_Bothell</th>\n",
       "      <th>city_Bremerton</th>\n",
       "      <th>city_Burlington</th>\n",
       "      <th>city_Chehalis</th>\n",
       "      <th>city_Chewelah</th>\n",
       "      <th>city_Coupeville</th>\n",
       "      <th>city_Des Moines</th>\n",
       "      <th>city_Everett</th>\n",
       "      <th>city_Federal Way</th>\n",
       "      <th>city_Ferndale</th>\n",
       "      <th>city_Friday Harbor</th>\n",
       "      <th>city_Kelso</th>\n",
       "      <th>city_Kennewick</th>\n",
       "      <th>city_Kirkland</th>\n",
       "      <th>city_La Center</th>\n",
       "      <th>city_Langley</th>\n",
       "      <th>city_Long Beach</th>\n",
       "      <th>city_Longview</th>\n",
       "      <th>city_Maple Valley</th>\n",
       "      <th>city_McKenna</th>\n",
       "      <th>city_Oakville</th>\n",
       "      <th>city_Ocean Shores</th>\n",
       "      <th>city_Olympia</th>\n",
       "      <th>city_Othello</th>\n",
       "      <th>city_Pasco</th>\n",
       "      <th>city_Port Angeles</th>\n",
       "      <th>city_Port Townsend</th>\n",
       "      <th>city_Pullman</th>\n",
       "      <th>city_Puyallup</th>\n",
       "      <th>city_Quilcene</th>\n",
       "      <th>city_Quincy</th>\n",
       "      <th>city_Raymond</th>\n",
       "      <th>city_Redmond</th>\n",
       "      <th>city_Republic</th>\n",
       "      <th>city_Roslyn</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Sequim</th>\n",
       "      <th>city_Spokane</th>\n",
       "      <th>city_Spokane Valley</th>\n",
       "      <th>city_Stanwood</th>\n",
       "      <th>city_Steilacoom</th>\n",
       "      <th>city_Sultan</th>\n",
       "      <th>city_Sumner</th>\n",
       "      <th>city_Tacoma</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Washougal</th>\n",
       "      <th>city_West Richland</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yakima</th>\n",
       "      <th>coat_Long</th>\n",
       "      <th>coat_Medium</th>\n",
       "      <th>coat_Short</th>\n",
       "      <th>coat_unknown</th>\n",
       "      <th>color_primary_Black</th>\n",
       "      <th>color_primary_Black &amp; White / Tuxedo</th>\n",
       "      <th>color_primary_Blue Cream</th>\n",
       "      <th>color_primary_Blue Point</th>\n",
       "      <th>color_primary_Brown / Chocolate</th>\n",
       "      <th>color_primary_Buff &amp; White</th>\n",
       "      <th>color_primary_Buff / Tan / Fawn</th>\n",
       "      <th>color_primary_Calico</th>\n",
       "      <th>color_primary_Chocolate Point</th>\n",
       "      <th>color_primary_Cream / Ivory</th>\n",
       "      <th>color_primary_Cream Point</th>\n",
       "      <th>color_primary_Dilute Calico</th>\n",
       "      <th>color_primary_Dilute Tortoiseshell</th>\n",
       "      <th>color_primary_Flame Point</th>\n",
       "      <th>color_primary_Gray &amp; White</th>\n",
       "      <th>color_primary_Gray / Blue / Silver</th>\n",
       "      <th>color_primary_Lilac Point</th>\n",
       "      <th>color_primary_Orange &amp; White</th>\n",
       "      <th>color_primary_Orange / Red</th>\n",
       "      <th>color_primary_Seal Point</th>\n",
       "      <th>color_primary_Smoke</th>\n",
       "      <th>color_primary_Tabby (Brown / Chocolate)</th>\n",
       "      <th>color_primary_Tabby (Buff / Tan / Fawn)</th>\n",
       "      <th>color_primary_Tabby (Gray / Blue / Silver)</th>\n",
       "      <th>color_primary_Tabby (Orange / Red)</th>\n",
       "      <th>color_primary_Tabby (Tiger Striped)</th>\n",
       "      <th>color_primary_Torbie</th>\n",
       "      <th>color_primary_Tortoiseshell</th>\n",
       "      <th>color_primary_White</th>\n",
       "      <th>color_primary_unknown</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>goodwith_cats_True</th>\n",
       "      <th>goodwith_cats_unknown</th>\n",
       "      <th>goodwith_children_True</th>\n",
       "      <th>goodwith_children_unknown</th>\n",
       "      <th>goodwith_dogs_True</th>\n",
       "      <th>goodwith_dogs_unknown</th>\n",
       "      <th>size_Large</th>\n",
       "      <th>size_Medium</th>\n",
       "      <th>size_Small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-1.49233</td>\n",
       "      <td>-5.38903</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.52865</td>\n",
       "      <td>-4.48741</td>\n",
       "      <td>-2.86887</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>-1.76561</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>-1.90570</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.75585</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.42834</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.30815</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>-0.25138</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>1.00801</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>0.23704</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>2.26947</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>5.20489</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>6.93423</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>7.19417</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>2.25044</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>16.41265</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>8.29302</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>16.07980</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>3.72142</td>\n",
       "      <td>2.91799</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>22.31936</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>14.43338</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>23.23252</td>\n",
       "      <td>17.54993</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>5.46809</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>3.12429</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>15.76632</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>6.56832</td>\n",
       "      <td>36.01111</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>4.37632</td>\n",
       "      <td>6.96666</td>\n",
       "      <td>7.22416</td>\n",
       "      <td>7.51164</td>\n",
       "      <td>14.20497</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>12.38951</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>10.62272</td>\n",
       "      <td>5.82126</td>\n",
       "      <td>11.46423</td>\n",
       "      <td>12.86019</td>\n",
       "      <td>11.58393</td>\n",
       "      <td>17.14510</td>\n",
       "      <td>5.49519</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>7.75996</td>\n",
       "      <td>6.45684</td>\n",
       "      <td>2.71930</td>\n",
       "      <td>7.25452</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>14.67310</td>\n",
       "      <td>5.90346</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>24.26745</td>\n",
       "      <td>4.29280</td>\n",
       "      <td>5.31317</td>\n",
       "      <td>7.83545</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>4.17548</td>\n",
       "      <td>18.45336</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>9.50760</td>\n",
       "      <td>6.41376</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>7.58020</td>\n",
       "      <td>9.86500</td>\n",
       "      <td>11.01971</td>\n",
       "      <td>7.04949</td>\n",
       "      <td>3.14445</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>2.20856</td>\n",
       "      <td>4.79931</td>\n",
       "      <td>3.32586</td>\n",
       "      <td>2.64598</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>5.28855</td>\n",
       "      <td>1.87333</td>\n",
       "      <td>2.85976</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>15.47040</td>\n",
       "      <td>5.44137</td>\n",
       "      <td>11.96662</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>5.72686</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>10.81581</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>12.69744</td>\n",
       "      <td>4.03782</td>\n",
       "      <td>4.20717</td>\n",
       "      <td>15.19046</td>\n",
       "      <td>5.27636</td>\n",
       "      <td>6.28938</td>\n",
       "      <td>9.18594</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>2.76206</td>\n",
       "      <td>7.72301</td>\n",
       "      <td>4.00953</td>\n",
       "      <td>4.73561</td>\n",
       "      <td>9.94137</td>\n",
       "      <td>8.03402</td>\n",
       "      <td>5.88675</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>4.16299</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>2.03515</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>3.39515</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>2.57568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       breed_mixed   declawed   distance   hasimage   hasvideo  house_trained  population  shots_current  spayed_neutered  special_needs  age_Adult   age_Baby  age_Young  breed_primary_Abyssinian  breed_primary_American Shorthair  breed_primary_Balinese  breed_primary_Bengal  breed_primary_Birman  breed_primary_Bombay  breed_primary_British Shorthair  breed_primary_Burmese  breed_primary_Calico  breed_primary_Chartreux  breed_primary_Devon Rex  breed_primary_Dilute Calico  breed_primary_Dilute Tortoiseshell  breed_primary_Domestic Long Hair  breed_primary_Domestic Medium Hair  breed_primary_Domestic Short Hair  breed_primary_Egyptian Mau  breed_primary_Exotic Shorthair  breed_primary_Extra-Toes Cat / Hemingway Polydactyl  breed_primary_Himalayan  breed_primary_Maine Coon  breed_primary_Manx  breed_primary_Munchkin  breed_primary_Norwegian Forest Cat  breed_primary_Persian  breed_primary_Pixiebob  breed_primary_Ragamuffin  breed_primary_Ragdoll  breed_primary_Russian Blue  breed_primary_Scottish Fold  breed_primary_Siamese  breed_primary_Silver  breed_primary_Singapura  breed_primary_Snowshoe  breed_primary_Tabby  breed_primary_Tiger  breed_primary_Tonkinese  breed_primary_Torbie  breed_primary_Tortoiseshell  breed_primary_Turkish Angora  breed_primary_Turkish Van  breed_primary_Tuxedo  city_Auburn  city_Bainbridge Island  city_Battle Ground  city_Bellingham  city_Bothell  city_Bremerton  city_Burlington  city_Chehalis  city_Chewelah  city_Coupeville  city_Des Moines  city_Everett  city_Federal Way  city_Ferndale  city_Friday Harbor  city_Kelso  city_Kennewick  city_Kirkland  city_La Center  city_Langley  city_Long Beach  city_Longview  city_Maple Valley  city_McKenna  city_Oakville  city_Ocean Shores  city_Olympia  city_Othello  city_Pasco  city_Port Angeles  city_Port Townsend  city_Pullman  city_Puyallup  city_Quilcene  city_Quincy  city_Raymond  city_Redmond  city_Republic  city_Roslyn  city_Seattle  city_Sequim  city_Spokane  city_Spokane Valley  city_Stanwood  city_Steilacoom  city_Sultan  city_Sumner  city_Tacoma  city_Vancouver  city_Washougal  city_West Richland  city_Woodinville  city_Yakima  coat_Long  coat_Medium  coat_Short  coat_unknown  color_primary_Black  color_primary_Black & White / Tuxedo  color_primary_Blue Cream  color_primary_Blue Point  color_primary_Brown / Chocolate  color_primary_Buff & White  color_primary_Buff / Tan / Fawn  color_primary_Calico  color_primary_Chocolate Point  color_primary_Cream / Ivory  color_primary_Cream Point  color_primary_Dilute Calico  color_primary_Dilute Tortoiseshell  color_primary_Flame Point  color_primary_Gray & White  color_primary_Gray / Blue / Silver  color_primary_Lilac Point  color_primary_Orange & White  color_primary_Orange / Red  color_primary_Seal Point  color_primary_Smoke  color_primary_Tabby (Brown / Chocolate)  color_primary_Tabby (Buff / Tan / Fawn)  color_primary_Tabby (Gray / Blue / Silver)  color_primary_Tabby (Orange / Red)  color_primary_Tabby (Tiger Striped)  color_primary_Torbie  color_primary_Tortoiseshell  color_primary_White  color_primary_unknown  gender_Female  goodwith_cats_True  goodwith_cats_unknown  goodwith_children_True  goodwith_children_unknown  goodwith_dogs_True  goodwith_dogs_unknown  size_Large  size_Medium  size_Small\n",
       "count   6489.00000 6489.00000 6489.00000 6489.00000 6489.00000     6489.00000  6489.00000     6489.00000       6489.00000     6489.00000 6489.00000 6489.00000 6489.00000                6489.00000                        6489.00000              6489.00000            6489.00000            6489.00000            6489.00000                       6489.00000             6489.00000            6489.00000               6489.00000               6489.00000                   6489.00000                          6489.00000                        6489.00000                          6489.00000                         6489.00000                  6489.00000                      6489.00000                                           6489.00000               6489.00000                6489.00000          6489.00000              6489.00000                          6489.00000             6489.00000              6489.00000                6489.00000             6489.00000                  6489.00000                   6489.00000             6489.00000            6489.00000               6489.00000              6489.00000           6489.00000           6489.00000               6489.00000            6489.00000                   6489.00000                    6489.00000                 6489.00000            6489.00000   6489.00000              6489.00000          6489.00000       6489.00000    6489.00000      6489.00000       6489.00000     6489.00000     6489.00000       6489.00000       6489.00000    6489.00000        6489.00000     6489.00000          6489.00000  6489.00000      6489.00000     6489.00000      6489.00000    6489.00000       6489.00000     6489.00000         6489.00000    6489.00000     6489.00000         6489.00000    6489.00000    6489.00000  6489.00000         6489.00000          6489.00000    6489.00000     6489.00000     6489.00000   6489.00000    6489.00000    6489.00000     6489.00000   6489.00000    6489.00000   6489.00000    6489.00000           6489.00000     6489.00000       6489.00000   6489.00000   6489.00000   6489.00000      6489.00000      6489.00000          6489.00000        6489.00000   6489.00000 6489.00000   6489.00000  6489.00000    6489.00000           6489.00000                            6489.00000                6489.00000                6489.00000                       6489.00000                  6489.00000                       6489.00000            6489.00000                     6489.00000                   6489.00000                 6489.00000                   6489.00000                          6489.00000                 6489.00000                  6489.00000                          6489.00000                 6489.00000                    6489.00000                  6489.00000                6489.00000           6489.00000                               6489.00000                               6489.00000                                  6489.00000                          6489.00000                           6489.00000            6489.00000                   6489.00000           6489.00000             6489.00000     6489.00000          6489.00000             6489.00000              6489.00000                 6489.00000          6489.00000             6489.00000  6489.00000   6489.00000  6489.00000\n",
       "mean      -0.00000    0.00000   -0.00000   -0.00000   -0.00000        0.00000    -0.00000        0.00000         -0.00000       -0.00000    0.00000    0.00000    0.00000                  -0.00000                           0.00000                -0.00000               0.00000              -0.00000               0.00000                         -0.00000                0.00000               0.00000                 -0.00000                 -0.00000                     -0.00000                            -0.00000                          -0.00000                            -0.00000                            0.00000                     0.00000                        -0.00000                                              0.00000                  0.00000                  -0.00000            -0.00000                 0.00000                            -0.00000               -0.00000                -0.00000                  -0.00000               -0.00000                    -0.00000                     -0.00000               -0.00000               0.00000                  0.00000                 0.00000             -0.00000             -0.00000                 -0.00000              -0.00000                     -0.00000                      -0.00000                   -0.00000              -0.00000     -0.00000                -0.00000             0.00000          0.00000       0.00000         0.00000         -0.00000       -0.00000       -0.00000         -0.00000         -0.00000       0.00000           0.00000       -0.00000            -0.00000    -0.00000         0.00000       -0.00000        -0.00000      -0.00000          0.00000       -0.00000           -0.00000       0.00000        0.00000           -0.00000      -0.00000      -0.00000     0.00000           -0.00000            -0.00000       0.00000       -0.00000        0.00000      0.00000      -0.00000      -0.00000        0.00000      0.00000       0.00000      0.00000       0.00000             -0.00000        0.00000         -0.00000     -0.00000     -0.00000     -0.00000         0.00000         0.00000             0.00000          -0.00000      0.00000    0.00000     -0.00000    -0.00000       0.00000              0.00000                               0.00000                  -0.00000                   0.00000                          0.00000                    -0.00000                         -0.00000               0.00000                       -0.00000                      0.00000                    0.00000                      0.00000                            -0.00000                   -0.00000                    -0.00000                            -0.00000                    0.00000                      -0.00000                    -0.00000                   0.00000             -0.00000                                 -0.00000                                 -0.00000                                    -0.00000                             0.00000                              0.00000               0.00000                      0.00000              0.00000                0.00000        0.00000             0.00000               -0.00000                -0.00000                   -0.00000            -0.00000                0.00000    -0.00000      0.00000     0.00000\n",
       "std        1.00008    1.00008    1.00008    1.00008    1.00008        1.00008     1.00008        1.00008          1.00008        1.00008    1.00008    1.00008    1.00008                   1.00008                           1.00008                 1.00008               1.00008               1.00008               1.00008                          1.00008                1.00008               1.00008                  1.00008                  1.00008                      1.00008                             1.00008                           1.00008                             1.00008                            1.00008                     1.00008                         1.00008                                              1.00008                  1.00008                   1.00008             1.00008                 1.00008                             1.00008                1.00008                 1.00008                   1.00008                1.00008                     1.00008                      1.00008                1.00008               1.00008                  1.00008                 1.00008              1.00008              1.00008                  1.00008               1.00008                      1.00008                       1.00008                    1.00008               1.00008      1.00008                 1.00008             1.00008          1.00008       1.00008         1.00008          1.00008        1.00008        1.00008          1.00008          1.00008       1.00008           1.00008        1.00008             1.00008     1.00008         1.00008        1.00008         1.00008       1.00008          1.00008        1.00008            1.00008       1.00008        1.00008            1.00008       1.00008       1.00008     1.00008            1.00008             1.00008       1.00008        1.00008        1.00008      1.00008       1.00008       1.00008        1.00008      1.00008       1.00008      1.00008       1.00008              1.00008        1.00008          1.00008      1.00008      1.00008      1.00008         1.00008         1.00008             1.00008           1.00008      1.00008    1.00008      1.00008     1.00008       1.00008              1.00008                               1.00008                   1.00008                   1.00008                          1.00008                     1.00008                          1.00008               1.00008                        1.00008                      1.00008                    1.00008                      1.00008                             1.00008                    1.00008                     1.00008                             1.00008                    1.00008                       1.00008                     1.00008                   1.00008              1.00008                                  1.00008                                  1.00008                                     1.00008                             1.00008                              1.00008               1.00008                      1.00008              1.00008                1.00008        1.00008             1.00008                1.00008                 1.00008                    1.00008             1.00008                1.00008     1.00008      1.00008     1.00008\n",
       "min       -1.52233   -0.09497   -1.49233   -5.38903   -0.19213       -1.26894    -0.52865       -4.48741         -2.86887       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793    -1.76561      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454     -1.90570    -0.38825\n",
       "25%       -1.52233   -0.09497   -0.75585    0.18556   -0.19213       -1.26894    -0.42834        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454      0.52474    -0.38825\n",
       "50%        0.65689   -0.09497   -0.30815    0.18556   -0.19213        0.78806    -0.25138        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730            -0.98213               -0.92987                -0.60340                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "75%        0.65689   -0.09497    1.00801    0.18556   -0.19213        0.78806     0.23704        0.22285          0.34857       -0.13900    1.40108    1.09020   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730             1.01820                1.07542                 1.65727                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "max        0.65689   10.52992    2.26947    0.18556    5.20489        0.78806     6.93423        0.22285          0.34857        7.19417    1.40108    1.09020    2.25044                  56.95173                          16.41265                56.95173              40.26475              46.49731              28.46269                         56.95173               80.54812               8.29302                 56.95173                 40.26475                     16.07980                            20.11374                           3.72142                             2.91799                            0.80174                    80.54812                        40.26475                                             22.31936                 28.46269                  14.43338            21.50581                80.54812                            56.95173               14.92510                46.49731                  56.95173               23.23252                    17.54993                     32.87096                5.46809              80.54812                 80.54812                17.98472              3.12429             46.49731                 56.95173              15.76632                      9.64590                      56.95173                   56.95173               6.56832     36.01111                32.87096            32.87096          4.37632       6.96666         7.22416          7.51164       14.20497       46.49731         12.38951         20.11374      10.62272           5.82126       11.46423            12.86019    11.58393        17.14510        5.49519        40.26475       7.75996          6.45684        2.71930            7.25452      21.50581       14.67310            5.90346      14.92510      24.26745     4.29280            5.31317             7.83545      30.43025        4.17548       18.45336     80.54812      56.95173       9.50760        6.41376     28.46269       7.58020      9.86500      11.01971              7.04949        3.14445         32.87096     14.92510     32.87096     30.43025        80.54812        80.54812            28.46269           2.20856      4.79931    3.32586      2.64598     0.56638       5.28855              1.87333                               2.85976                  17.98472                  15.47040                          5.44137                    11.96662                          9.64590               5.72686                       17.98472                     10.52992                   19.51169                     10.81581                             9.64590                   12.69744                     4.03782                             4.20717                   15.19046                       5.27636                     6.28938                   9.18594             19.51169                                  2.76206                                  7.72301                                     4.00953                             4.73561                              9.94137               8.03402                      5.88675              9.64590                4.16299        0.95730             1.01820                1.07542                 1.65727                    0.73053             2.03515                0.58441     3.39515      0.52474     2.57568"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X = pd.DataFrame(scaled, columns=X.columns)\n",
    "scaled_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.3, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542, 152), (1947, 152))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542,), (1947,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_test.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_test.csv\"\n"
     ]
    }
   ],
   "source": [
    "# save training and test sets\n",
    "datapath = 'data/tt_sets'\n",
    "save_file(X_train, 'cats_X_train.csv', datapath)\n",
    "save_file(X_test, 'cats_X_test.csv', datapath)\n",
    "save_file(y_train, 'cats_y_train.csv', datapath)\n",
    "save_file(y_test, 'cats_y_test.csv', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "### Goal: Build two to three different models and identify the best one.\n",
    "<ul><li>Fit your models with a training dataset</li>\n",
    "<li>Review model outcomes â Iterate over additional models as needed</li>\n",
    "<li>Identify the final model that you think is the best model for this project</li></ul>\n",
    " Review the following questions and apply them to your analysis: \n",
    "<ul><li>Does my data involve a time series or forecasting? If so, am I splitting the train and test data appropriately?</li>\n",
    "<li>Is my response variable continuous or categorical?</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7832782367335216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10267818, 0.06431094, 0.11979787, 0.07698541, 0.14252197])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_scores_preopt = rf_cv['test_score']\n",
    "rf_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10125887423677346, 0.02829382379509712)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_cv_scores_preopt), np.std(rf_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  34.435340\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)\n",
    "rmse_rf_preopt = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "print(\"RMSE : % f\" %(rmse_rf_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "rf_grid_params = {\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3,4,5, 6,7,8,9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'n_estimators': 297}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gcv = GridSearchCV(rf, param_grid=rf_grid_params, cv=5, n_jobs=-1)\n",
    "gcv.fit(X_train, y_train)\n",
    "gcv_params = gcv.best_params_\n",
    "\n",
    "gcv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4145858789830231"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=gcv_params['n_estimators'], max_depth=gcv_params['max_depth'])\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16474897, 0.15439094, 0.17696468, 0.15925713, 0.19595176]),\n",
       " array([0.10877302, 0.12327886, 0.0328471 , 0.07556306, 0.16593833]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_train = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_test = cross_validate(rf, X_test, y_test, cv=5)\n",
    "rf_cv_train['test_score'], rf_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.17026269550813455\n",
      "Average CV Score, Trest Set: 0.10128007258635516\n"
     ]
    }
   ],
   "source": [
    "rf_train_score = np.mean(rf_cv_train['test_score'])\n",
    "rf_test_score = np.mean(rf_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", rf_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", rf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  27.596311\n",
      "RMSE Test Set :  32.374979\n"
     ]
    }
   ],
   "source": [
    "rf_train_pred = rf.predict(X_train)\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "rf_rmse_train = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_rmse_test = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(rf_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(rf_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31787013710575396"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18857688, 0.18959194, 0.176784  , 0.18527591, 0.18640978])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_scores_preopt = gb_cv['test_score']\n",
    "gb_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18532770274172977, 0.004536720917895641)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gb_cv_scores_preopt), np.std(gb_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  32.265180\n"
     ]
    }
   ],
   "source": [
    "gb_pred = gb.predict(X_test)\n",
    "rmse_gb_preopt = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "print(\"RMSE : % f\" %(rmse_gb_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "gb_grid_params = {\n",
    "        'learning_rate': [.01, .1, 1],\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n",
       "                         'n_estimators': [10, 12, 16, 20, 26, 33, 42, 54, 69,\n",
       "                                          88, 112, 143, 183, 233, 297, 379, 483,\n",
       "                                          615, 784, 1000]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_cv = GridSearchCV(gb, param_grid=gb_grid_params, cv=5, n_jobs=-1)\n",
    "gb_grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 297}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_cv.fit(X_train, y_train)\n",
    "gb_grid_cv_params = gb_grid_cv.best_params_\n",
    "\n",
    "gb_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31766102768246784"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=gb_grid_cv_params['n_estimators'], max_depth=gb_grid_cv_params['max_depth'], learning_rate=gb_grid_cv_params['learning_rate'])\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.19890417, 0.19264768, 0.1885202 , 0.2071238 , 0.19943449]),\n",
       " array([0.19627885, 0.12275872, 0.04499154, 0.0828051 , 0.18056996]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv_train = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_test = cross_validate(gb, X_test, y_test, cv=5)\n",
    "gb_cv_train['test_score'], gb_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.19732606572179417\n",
      "Average CV Score, Trest Set: 0.1254808327516697\n"
     ]
    }
   ],
   "source": [
    "gb_train_score = np.mean(gb_cv_train['test_score'])\n",
    "gb_test_score = np.mean(gb_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", gb_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", gb_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  29.793362\n",
      "RMSE Test Set :  32.167483\n"
     ]
    }
   ],
   "source": [
    "gb_train_pred = gb.predict(X_train)\n",
    "gb_test_pred = gb.predict(X_test)\n",
    "gb_rmse_train = np.sqrt(mean_squared_error(y_train, gb_train_pred))\n",
    "gb_rmse_test = np.sqrt(mean_squared_error(y_test, gb_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(gb_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(gb_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8760428716103522"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=25, weights='distance')\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08815071889321772, 0.018072258030583113)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_cv = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_scores_preopt = kn_cv['test_score']\n",
    "np.mean(kn_cv_scores_preopt), np.std(kn_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  34.766549\n"
     ]
    }
   ],
   "source": [
    "kn_pred = kn.predict(X_test)\n",
    "rmse_kn_preopt = np.sqrt(mean_squared_error(y_test, kn_pred))\n",
    "print(\"RMSE : % f\" %(rmse_kn_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "kn_grid_params = {\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'n_neighbors': n_est,\n",
    "        'p': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 12, 'p': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_grid_cv = GridSearchCV(kn, param_grid=kn_grid_params, cv=5, n_jobs=-1)\n",
    "kn_grid_cv.fit(X_train, y_train)\n",
    "kn_grid_cv_params = kn_grid_cv.best_params_\n",
    "\n",
    "kn_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2893853844980272"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=kn_grid_cv_params['n_neighbors'], weights=kn_grid_cv_params['weights'], p=kn_grid_cv_params['p'])\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.12270817, 0.15985082, 0.15388799, 0.12599193, 0.15656884]),\n",
       " array([ 0.05678765, -0.00627876,  0.06919624,  0.00615119,  0.10881523]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_cv_train = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_test = cross_validate(kn, X_test, y_test, cv=5)\n",
    "kn_cv_train['test_score'], kn_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.14380155080470985\n",
      "Average CV Score, Trest Set: 0.04693430907128959\n"
     ]
    }
   ],
   "source": [
    "kn_train_score = np.mean(kn_cv_train['test_score'])\n",
    "kn_test_score = np.mean(kn_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", kn_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", kn_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  30.404404\n",
      "RMSE Test Set :  33.158672\n"
     ]
    }
   ],
   "source": [
    "kn_train_pred = kn.predict(X_train)\n",
    "kn_test_pred = kn.predict(X_test)\n",
    "kn_rmse_train = np.sqrt(mean_squared_error(y_train, kn_train_pred))\n",
    "kn_rmse_test = np.sqrt(mean_squared_error(y_test, kn_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(kn_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(kn_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5458485737696644"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 50)\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15601831631480173, 0.02676679102213802)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_cv = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_scores_preopt = xg_cv['test_score']\n",
    "np.mean(xg_cv_scores_preopt), np.std(xg_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  32.830385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xg_pred = xg.predict(X_test)\n",
    "rmse_xg_preopt = np.sqrt(mean_squared_error(y_test, xg_pred))\n",
    "print(\"RMSE : % f\" %(rmse_xg_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "xg_grid_params = {\n",
    "        'objective': ['reg:squarederror', 'reg:squaredlogerror', 'reg:logistic'],\n",
    "        'n_estimators': n_est,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20, 'objective': 'reg:squarederror'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_grid_cv = GridSearchCV(xg, param_grid=xg_grid_params, cv=5, n_jobs=-1)\n",
    "xg_grid_cv.fit(X_train, y_train)\n",
    "xg_grid_cv_params = xg_grid_cv.best_params_\n",
    "\n",
    "xg_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4250868532823179"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = xgb.XGBRegressor(objective=xg_grid_cv_params['objective'], n_estimators = xg_grid_cv_params['n_estimators'])\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.17946805, 0.16315177, 0.20408199, 0.15520235, 0.17521979]),\n",
       " array([0.15223908, 0.12042808, 0.00786487, 0.05927666, 0.17337777]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_cv_train = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_test = cross_validate(xg, X_test, y_test, cv=5)\n",
    "xg_cv_train['test_score'], xg_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.17542478958428956\n",
      "Average CV Score, Trest Set: 0.10263729281136573\n"
     ]
    }
   ],
   "source": [
    "xg_train_score = np.mean(xg_cv_train['test_score'])\n",
    "xg_test_score = np.mean(xg_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", xg_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", xg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  27.347684\n",
      "RMSE Test Set :  32.374172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xg_train_pred = xg.predict(X_train)\n",
    "xg_test_pred = xg.predict(X_test)\n",
    "xg_rmse_train = np.sqrt(mean_squared_error(y_train, xg_train_pred))\n",
    "xg_rmse_test = np.sqrt(mean_squared_error(y_test, xg_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(xg_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(xg_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV Train</th>\n",
       "      <th>RMSE Train</th>\n",
       "      <th>CV Test</th>\n",
       "      <th>RMSE Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.17026</td>\n",
       "      <td>27.59631</td>\n",
       "      <td>0.10128</td>\n",
       "      <td>32.37498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>0.19733</td>\n",
       "      <td>29.79336</td>\n",
       "      <td>0.12548</td>\n",
       "      <td>32.16748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNeighbors</th>\n",
       "      <td>0.14380</td>\n",
       "      <td>30.40440</td>\n",
       "      <td>0.04693</td>\n",
       "      <td>33.15867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.17542</td>\n",
       "      <td>27.34768</td>\n",
       "      <td>0.10264</td>\n",
       "      <td>32.37417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CV Train  RMSE Train  CV Test  RMSE Test\n",
       "RandomForest       0.17026    27.59631  0.10128   32.37498\n",
       "GradientBoosting   0.19733    29.79336  0.12548   32.16748\n",
       "KNNeighbors        0.14380    30.40440  0.04693   33.15867\n",
       "XGBoost            0.17542    27.34768  0.10264   32.37417"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame({'CV Train': [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score)], 'RMSE Train': [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train],'CV Test': [np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)], 'RMSE Test': [rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]}, index=['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost'])\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best CV Score: \n",
      "Train: KNNeighbors \n",
      "Test: KNNeighbors\n",
      "\n",
      "Model with best RMSE: \n",
      "Train: XGBoost \n",
      "Test: GradientBoosting\n"
     ]
    }
   ],
   "source": [
    "print(\"Model with best CV Score: \\nTrain:\", model_scores['CV Train'].idxmin(), \"\\nTest:\", model_scores['CV Test'].idxmin())\n",
    "print(\"\\nModel with best RMSE: \\nTrain:\", model_scores['RMSE Train'].idxmin(), \"\\nTest:\", model_scores['RMSE Test'].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost','RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost']\n",
    "cv_scores_all = [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score), np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)]\n",
    "types = ['train', 'train', 'train', 'train', 'test', 'test', 'test', 'test']\n",
    "rmse_scores_all = [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train, rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]\n",
    "\n",
    "cv_scores = pd.DataFrame(list(zip(models, cv_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) \n",
    "rmse_scores = pd.DataFrame(list(zip(models, rmse_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=cv_scores)\n",
    "plt.title(\"CV Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=rmse_scores)\n",
    "plt.title(\"RMSE Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go forward with **GradientBoosting** for the cats data. I'm choosing this model because it has the best RMSE score for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.490261099806496"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_test = np.std(y_test)\n",
    "std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_scores_test = model_scores.drop(columns=['CV Train','RMSE Train']).rename(columns={'CV Test':'R^2', 'RMSE Test':'RMSE'})\n",
    "\n",
    "#standardize the RMSE by dividing it by the standard deviation of y_test\n",
    "model_scores_test['RMSE'] = model_scores_test['RMSE']/std_test\n",
    "\n",
    "#sort scores by lowest RMSE\n",
    "model_scores_test.sort_values(by='RMSE', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=np.round(model_scores_test.values,4), \n",
    "    colLabels=model_scores_test.columns, \n",
    "    rowLabels=model_scores_test.index, \n",
    "    loc='center', \n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.scale(.75, 3)\n",
    "\n",
    "plt.title('Model Metrics: Cats', size=16)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('assets/cats_model_metrics.png', bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
