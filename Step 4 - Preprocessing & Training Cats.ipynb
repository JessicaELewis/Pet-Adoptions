{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Training\n",
    "\n",
    "### Goal:\n",
    "<p>Create a cleaned development dataset you can use to complete the modeling step of your project.</p>\n",
    "\n",
    "### Steps:\n",
    "<ul><li>Create dummy or indicator features for categorical variables</li><li>Standardize the magnitude of numeric features using a scaler</li><li>Split into testing and training datasets</li></ul>\n",
    "Review the following questions and apply them to your dataset:<ul><li>Does my data set have any categorical data, such as Gender or day of the week?</li><li>Do my features have data values that range from 0 - 100 or 0-1 or both and more? Â </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6489 entries, 0 to 6488\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   duration_as_adoptable  6489 non-null   float64\n",
      " 1   age                    6489 non-null   object \n",
      " 2   breed_mixed            6489 non-null   bool   \n",
      " 3   breed_primary          6489 non-null   object \n",
      " 4   city                   6489 non-null   object \n",
      " 5   coat                   6489 non-null   object \n",
      " 6   color_primary          6489 non-null   object \n",
      " 7   declawed               6489 non-null   bool   \n",
      " 8   distance               6489 non-null   float64\n",
      " 9   gender                 6489 non-null   object \n",
      " 10  goodwith_cats          6489 non-null   object \n",
      " 11  goodwith_children      6489 non-null   object \n",
      " 12  goodwith_dogs          6489 non-null   object \n",
      " 13  hasimage               6489 non-null   bool   \n",
      " 14  hasvideo               6489 non-null   bool   \n",
      " 15  house_trained          6489 non-null   bool   \n",
      " 16  population             6489 non-null   float64\n",
      " 17  shots_current          6489 non-null   bool   \n",
      " 18  size                   6489 non-null   object \n",
      " 19  spayed_neutered        6489 non-null   bool   \n",
      " 20  special_needs          6489 non-null   bool   \n",
      "dtypes: bool(8), float64(3), object(10)\n",
      "memory usage: 709.9+ KB\n"
     ]
    }
   ],
   "source": [
    "adopted = pd.read_csv('data/cats_trimmed.csv')\n",
    "adopted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies!\n",
    "### After converting bools to ints, of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adopted\n",
    "df.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']] = adopted.loc[:, ['breed_mixed', 'declawed', 'hasimage', 'hasvideo', 'house_trained', 'shots_current', 'spayed_neutered', 'special_needs']].astype('int64')\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop one of each of the dummy category columns so those features don't double-weight anything\n",
    "df.drop(['age_Senior', 'gender_Male', 'size_Extra Large', 'coat_Hairless', 'breed_primary_American Bobtail', 'color_primary_Tabby (Leopard / Spotted)', 'goodwith_children_False', 'goodwith_dogs_False', 'goodwith_cats_False', 'city_Lacey'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed = imp.fit_transform(df)\n",
    "df = pd.DataFrame(imputed, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='duration_as_adoptable')\n",
    "y = df.duration_as_adoptable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed_mixed</th>\n",
       "      <th>declawed</th>\n",
       "      <th>distance</th>\n",
       "      <th>hasimage</th>\n",
       "      <th>hasvideo</th>\n",
       "      <th>house_trained</th>\n",
       "      <th>population</th>\n",
       "      <th>shots_current</th>\n",
       "      <th>spayed_neutered</th>\n",
       "      <th>special_needs</th>\n",
       "      <th>age_Adult</th>\n",
       "      <th>age_Baby</th>\n",
       "      <th>age_Young</th>\n",
       "      <th>breed_primary_Abyssinian</th>\n",
       "      <th>breed_primary_American Shorthair</th>\n",
       "      <th>breed_primary_Balinese</th>\n",
       "      <th>breed_primary_Bengal</th>\n",
       "      <th>breed_primary_Birman</th>\n",
       "      <th>breed_primary_Bombay</th>\n",
       "      <th>breed_primary_British Shorthair</th>\n",
       "      <th>breed_primary_Burmese</th>\n",
       "      <th>breed_primary_Calico</th>\n",
       "      <th>breed_primary_Chartreux</th>\n",
       "      <th>breed_primary_Devon Rex</th>\n",
       "      <th>breed_primary_Dilute Calico</th>\n",
       "      <th>breed_primary_Dilute Tortoiseshell</th>\n",
       "      <th>breed_primary_Domestic Long Hair</th>\n",
       "      <th>breed_primary_Domestic Medium Hair</th>\n",
       "      <th>breed_primary_Domestic Short Hair</th>\n",
       "      <th>breed_primary_Egyptian Mau</th>\n",
       "      <th>breed_primary_Exotic Shorthair</th>\n",
       "      <th>breed_primary_Extra-Toes Cat / Hemingway Polydactyl</th>\n",
       "      <th>breed_primary_Himalayan</th>\n",
       "      <th>breed_primary_Maine Coon</th>\n",
       "      <th>breed_primary_Manx</th>\n",
       "      <th>breed_primary_Munchkin</th>\n",
       "      <th>breed_primary_Norwegian Forest Cat</th>\n",
       "      <th>breed_primary_Persian</th>\n",
       "      <th>breed_primary_Pixiebob</th>\n",
       "      <th>breed_primary_Ragamuffin</th>\n",
       "      <th>breed_primary_Ragdoll</th>\n",
       "      <th>breed_primary_Russian Blue</th>\n",
       "      <th>breed_primary_Scottish Fold</th>\n",
       "      <th>breed_primary_Siamese</th>\n",
       "      <th>breed_primary_Silver</th>\n",
       "      <th>breed_primary_Singapura</th>\n",
       "      <th>breed_primary_Snowshoe</th>\n",
       "      <th>breed_primary_Tabby</th>\n",
       "      <th>breed_primary_Tiger</th>\n",
       "      <th>breed_primary_Tonkinese</th>\n",
       "      <th>breed_primary_Torbie</th>\n",
       "      <th>breed_primary_Tortoiseshell</th>\n",
       "      <th>breed_primary_Turkish Angora</th>\n",
       "      <th>breed_primary_Turkish Van</th>\n",
       "      <th>breed_primary_Tuxedo</th>\n",
       "      <th>city_Auburn</th>\n",
       "      <th>city_Bainbridge Island</th>\n",
       "      <th>city_Battle Ground</th>\n",
       "      <th>city_Bellingham</th>\n",
       "      <th>city_Bothell</th>\n",
       "      <th>city_Bremerton</th>\n",
       "      <th>city_Burlington</th>\n",
       "      <th>city_Chehalis</th>\n",
       "      <th>city_Chewelah</th>\n",
       "      <th>city_Coupeville</th>\n",
       "      <th>city_Des Moines</th>\n",
       "      <th>city_Everett</th>\n",
       "      <th>city_Federal Way</th>\n",
       "      <th>city_Ferndale</th>\n",
       "      <th>city_Friday Harbor</th>\n",
       "      <th>city_Kelso</th>\n",
       "      <th>city_Kennewick</th>\n",
       "      <th>city_Kirkland</th>\n",
       "      <th>city_La Center</th>\n",
       "      <th>city_Langley</th>\n",
       "      <th>city_Long Beach</th>\n",
       "      <th>city_Longview</th>\n",
       "      <th>city_Maple Valley</th>\n",
       "      <th>city_McKenna</th>\n",
       "      <th>city_Oakville</th>\n",
       "      <th>city_Ocean Shores</th>\n",
       "      <th>city_Olympia</th>\n",
       "      <th>city_Othello</th>\n",
       "      <th>city_Pasco</th>\n",
       "      <th>city_Port Angeles</th>\n",
       "      <th>city_Port Townsend</th>\n",
       "      <th>city_Pullman</th>\n",
       "      <th>city_Puyallup</th>\n",
       "      <th>city_Quilcene</th>\n",
       "      <th>city_Quincy</th>\n",
       "      <th>city_Raymond</th>\n",
       "      <th>city_Redmond</th>\n",
       "      <th>city_Republic</th>\n",
       "      <th>city_Roslyn</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Sequim</th>\n",
       "      <th>city_Spokane</th>\n",
       "      <th>city_Spokane Valley</th>\n",
       "      <th>city_Stanwood</th>\n",
       "      <th>city_Steilacoom</th>\n",
       "      <th>city_Sultan</th>\n",
       "      <th>city_Sumner</th>\n",
       "      <th>city_Tacoma</th>\n",
       "      <th>city_Vancouver</th>\n",
       "      <th>city_Washougal</th>\n",
       "      <th>city_West Richland</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yakima</th>\n",
       "      <th>coat_Long</th>\n",
       "      <th>coat_Medium</th>\n",
       "      <th>coat_Short</th>\n",
       "      <th>coat_unknown</th>\n",
       "      <th>color_primary_Black</th>\n",
       "      <th>color_primary_Black &amp; White / Tuxedo</th>\n",
       "      <th>color_primary_Blue Cream</th>\n",
       "      <th>color_primary_Blue Point</th>\n",
       "      <th>color_primary_Brown / Chocolate</th>\n",
       "      <th>color_primary_Buff &amp; White</th>\n",
       "      <th>color_primary_Buff / Tan / Fawn</th>\n",
       "      <th>color_primary_Calico</th>\n",
       "      <th>color_primary_Chocolate Point</th>\n",
       "      <th>color_primary_Cream / Ivory</th>\n",
       "      <th>color_primary_Cream Point</th>\n",
       "      <th>color_primary_Dilute Calico</th>\n",
       "      <th>color_primary_Dilute Tortoiseshell</th>\n",
       "      <th>color_primary_Flame Point</th>\n",
       "      <th>color_primary_Gray &amp; White</th>\n",
       "      <th>color_primary_Gray / Blue / Silver</th>\n",
       "      <th>color_primary_Lilac Point</th>\n",
       "      <th>color_primary_Orange &amp; White</th>\n",
       "      <th>color_primary_Orange / Red</th>\n",
       "      <th>color_primary_Seal Point</th>\n",
       "      <th>color_primary_Smoke</th>\n",
       "      <th>color_primary_Tabby (Brown / Chocolate)</th>\n",
       "      <th>color_primary_Tabby (Buff / Tan / Fawn)</th>\n",
       "      <th>color_primary_Tabby (Gray / Blue / Silver)</th>\n",
       "      <th>color_primary_Tabby (Orange / Red)</th>\n",
       "      <th>color_primary_Tabby (Tiger Striped)</th>\n",
       "      <th>color_primary_Torbie</th>\n",
       "      <th>color_primary_Tortoiseshell</th>\n",
       "      <th>color_primary_White</th>\n",
       "      <th>color_primary_unknown</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>goodwith_cats_True</th>\n",
       "      <th>goodwith_cats_unknown</th>\n",
       "      <th>goodwith_children_True</th>\n",
       "      <th>goodwith_children_unknown</th>\n",
       "      <th>goodwith_dogs_True</th>\n",
       "      <th>goodwith_dogs_unknown</th>\n",
       "      <th>size_Large</th>\n",
       "      <th>size_Medium</th>\n",
       "      <th>size_Small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "      <td>6489.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "      <td>1.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-1.49233</td>\n",
       "      <td>-5.38903</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.52865</td>\n",
       "      <td>-4.48741</td>\n",
       "      <td>-2.86887</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>-1.76561</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>-1.90570</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.52233</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.75585</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>-1.26894</td>\n",
       "      <td>-0.42834</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>-1.24729</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>-1.04461</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>-1.36886</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>-1.71113</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.30815</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>-0.25138</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>-0.71374</td>\n",
       "      <td>-0.91726</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>-0.98213</td>\n",
       "      <td>-0.92987</td>\n",
       "      <td>-0.60340</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>1.00801</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>-0.19213</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>0.23704</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>-0.13900</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>-0.44436</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06093</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.12058</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.06219</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.26871</td>\n",
       "      <td>-0.34270</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.04480</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.06928</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.04304</td>\n",
       "      <td>-0.05698</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.18288</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.32007</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.15225</td>\n",
       "      <td>-0.02777</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.22850</td>\n",
       "      <td>-0.14354</td>\n",
       "      <td>-0.13842</td>\n",
       "      <td>-0.13313</td>\n",
       "      <td>-0.07040</td>\n",
       "      <td>-0.02151</td>\n",
       "      <td>-0.08071</td>\n",
       "      <td>-0.04972</td>\n",
       "      <td>-0.09414</td>\n",
       "      <td>-0.17178</td>\n",
       "      <td>-0.08723</td>\n",
       "      <td>-0.07776</td>\n",
       "      <td>-0.08633</td>\n",
       "      <td>-0.05833</td>\n",
       "      <td>-0.18198</td>\n",
       "      <td>-0.02484</td>\n",
       "      <td>-0.12887</td>\n",
       "      <td>-0.15487</td>\n",
       "      <td>-0.36774</td>\n",
       "      <td>-0.13785</td>\n",
       "      <td>-0.04650</td>\n",
       "      <td>-0.06815</td>\n",
       "      <td>-0.16939</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.04121</td>\n",
       "      <td>-0.23295</td>\n",
       "      <td>-0.18821</td>\n",
       "      <td>-0.12763</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.23949</td>\n",
       "      <td>-0.05419</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01756</td>\n",
       "      <td>-0.10518</td>\n",
       "      <td>-0.15591</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.13192</td>\n",
       "      <td>-0.10137</td>\n",
       "      <td>-0.09075</td>\n",
       "      <td>-0.14185</td>\n",
       "      <td>-0.31802</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.06700</td>\n",
       "      <td>-0.03042</td>\n",
       "      <td>-0.03286</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>-0.03513</td>\n",
       "      <td>-0.45278</td>\n",
       "      <td>-0.20836</td>\n",
       "      <td>-0.30067</td>\n",
       "      <td>-0.37793</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>-0.18909</td>\n",
       "      <td>-0.53381</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.06464</td>\n",
       "      <td>-0.18378</td>\n",
       "      <td>-0.08357</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.17462</td>\n",
       "      <td>-0.05560</td>\n",
       "      <td>-0.09497</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.09246</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.07876</td>\n",
       "      <td>-0.24766</td>\n",
       "      <td>-0.23769</td>\n",
       "      <td>-0.06583</td>\n",
       "      <td>-0.18952</td>\n",
       "      <td>-0.15900</td>\n",
       "      <td>-0.10886</td>\n",
       "      <td>-0.05125</td>\n",
       "      <td>-0.36205</td>\n",
       "      <td>-0.12948</td>\n",
       "      <td>-0.24941</td>\n",
       "      <td>-0.21117</td>\n",
       "      <td>-0.10059</td>\n",
       "      <td>-0.12447</td>\n",
       "      <td>-0.16987</td>\n",
       "      <td>-0.10367</td>\n",
       "      <td>-0.24021</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>-0.49136</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>-0.29454</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>-0.38825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>2.26947</td>\n",
       "      <td>0.18556</td>\n",
       "      <td>5.20489</td>\n",
       "      <td>0.78806</td>\n",
       "      <td>6.93423</td>\n",
       "      <td>0.22285</td>\n",
       "      <td>0.34857</td>\n",
       "      <td>7.19417</td>\n",
       "      <td>1.40108</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>2.25044</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>16.41265</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>8.29302</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>16.07980</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>3.72142</td>\n",
       "      <td>2.91799</td>\n",
       "      <td>0.80174</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>22.31936</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>14.43338</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>23.23252</td>\n",
       "      <td>17.54993</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>5.46809</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>3.12429</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>15.76632</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>6.56832</td>\n",
       "      <td>36.01111</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>4.37632</td>\n",
       "      <td>6.96666</td>\n",
       "      <td>7.22416</td>\n",
       "      <td>7.51164</td>\n",
       "      <td>14.20497</td>\n",
       "      <td>46.49731</td>\n",
       "      <td>12.38951</td>\n",
       "      <td>20.11374</td>\n",
       "      <td>10.62272</td>\n",
       "      <td>5.82126</td>\n",
       "      <td>11.46423</td>\n",
       "      <td>12.86019</td>\n",
       "      <td>11.58393</td>\n",
       "      <td>17.14510</td>\n",
       "      <td>5.49519</td>\n",
       "      <td>40.26475</td>\n",
       "      <td>7.75996</td>\n",
       "      <td>6.45684</td>\n",
       "      <td>2.71930</td>\n",
       "      <td>7.25452</td>\n",
       "      <td>21.50581</td>\n",
       "      <td>14.67310</td>\n",
       "      <td>5.90346</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>24.26745</td>\n",
       "      <td>4.29280</td>\n",
       "      <td>5.31317</td>\n",
       "      <td>7.83545</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>4.17548</td>\n",
       "      <td>18.45336</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>56.95173</td>\n",
       "      <td>9.50760</td>\n",
       "      <td>6.41376</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>7.58020</td>\n",
       "      <td>9.86500</td>\n",
       "      <td>11.01971</td>\n",
       "      <td>7.04949</td>\n",
       "      <td>3.14445</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>14.92510</td>\n",
       "      <td>32.87096</td>\n",
       "      <td>30.43025</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>80.54812</td>\n",
       "      <td>28.46269</td>\n",
       "      <td>2.20856</td>\n",
       "      <td>4.79931</td>\n",
       "      <td>3.32586</td>\n",
       "      <td>2.64598</td>\n",
       "      <td>0.56638</td>\n",
       "      <td>5.28855</td>\n",
       "      <td>1.87333</td>\n",
       "      <td>2.85976</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>15.47040</td>\n",
       "      <td>5.44137</td>\n",
       "      <td>11.96662</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>5.72686</td>\n",
       "      <td>17.98472</td>\n",
       "      <td>10.52992</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>10.81581</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>12.69744</td>\n",
       "      <td>4.03782</td>\n",
       "      <td>4.20717</td>\n",
       "      <td>15.19046</td>\n",
       "      <td>5.27636</td>\n",
       "      <td>6.28938</td>\n",
       "      <td>9.18594</td>\n",
       "      <td>19.51169</td>\n",
       "      <td>2.76206</td>\n",
       "      <td>7.72301</td>\n",
       "      <td>4.00953</td>\n",
       "      <td>4.73561</td>\n",
       "      <td>9.94137</td>\n",
       "      <td>8.03402</td>\n",
       "      <td>5.88675</td>\n",
       "      <td>9.64590</td>\n",
       "      <td>4.16299</td>\n",
       "      <td>0.95730</td>\n",
       "      <td>1.01820</td>\n",
       "      <td>1.07542</td>\n",
       "      <td>1.65727</td>\n",
       "      <td>0.73053</td>\n",
       "      <td>2.03515</td>\n",
       "      <td>0.58441</td>\n",
       "      <td>3.39515</td>\n",
       "      <td>0.52474</td>\n",
       "      <td>2.57568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       breed_mixed   declawed   distance   hasimage   hasvideo  house_trained  population  shots_current  spayed_neutered  special_needs  age_Adult   age_Baby  age_Young  breed_primary_Abyssinian  breed_primary_American Shorthair  breed_primary_Balinese  breed_primary_Bengal  breed_primary_Birman  breed_primary_Bombay  breed_primary_British Shorthair  breed_primary_Burmese  breed_primary_Calico  breed_primary_Chartreux  breed_primary_Devon Rex  breed_primary_Dilute Calico  breed_primary_Dilute Tortoiseshell  breed_primary_Domestic Long Hair  breed_primary_Domestic Medium Hair  breed_primary_Domestic Short Hair  breed_primary_Egyptian Mau  breed_primary_Exotic Shorthair  breed_primary_Extra-Toes Cat / Hemingway Polydactyl  breed_primary_Himalayan  breed_primary_Maine Coon  breed_primary_Manx  breed_primary_Munchkin  breed_primary_Norwegian Forest Cat  breed_primary_Persian  breed_primary_Pixiebob  breed_primary_Ragamuffin  breed_primary_Ragdoll  breed_primary_Russian Blue  breed_primary_Scottish Fold  breed_primary_Siamese  breed_primary_Silver  breed_primary_Singapura  breed_primary_Snowshoe  breed_primary_Tabby  breed_primary_Tiger  breed_primary_Tonkinese  breed_primary_Torbie  breed_primary_Tortoiseshell  breed_primary_Turkish Angora  breed_primary_Turkish Van  breed_primary_Tuxedo  city_Auburn  city_Bainbridge Island  city_Battle Ground  city_Bellingham  city_Bothell  city_Bremerton  city_Burlington  city_Chehalis  city_Chewelah  city_Coupeville  city_Des Moines  city_Everett  city_Federal Way  city_Ferndale  city_Friday Harbor  city_Kelso  city_Kennewick  city_Kirkland  city_La Center  city_Langley  city_Long Beach  city_Longview  city_Maple Valley  city_McKenna  city_Oakville  city_Ocean Shores  city_Olympia  city_Othello  city_Pasco  city_Port Angeles  city_Port Townsend  city_Pullman  city_Puyallup  city_Quilcene  city_Quincy  city_Raymond  city_Redmond  city_Republic  city_Roslyn  city_Seattle  city_Sequim  city_Spokane  city_Spokane Valley  city_Stanwood  city_Steilacoom  city_Sultan  city_Sumner  city_Tacoma  city_Vancouver  city_Washougal  city_West Richland  city_Woodinville  city_Yakima  coat_Long  coat_Medium  coat_Short  coat_unknown  color_primary_Black  color_primary_Black & White / Tuxedo  color_primary_Blue Cream  color_primary_Blue Point  color_primary_Brown / Chocolate  color_primary_Buff & White  color_primary_Buff / Tan / Fawn  color_primary_Calico  color_primary_Chocolate Point  color_primary_Cream / Ivory  color_primary_Cream Point  color_primary_Dilute Calico  color_primary_Dilute Tortoiseshell  color_primary_Flame Point  color_primary_Gray & White  color_primary_Gray / Blue / Silver  color_primary_Lilac Point  color_primary_Orange & White  color_primary_Orange / Red  color_primary_Seal Point  color_primary_Smoke  color_primary_Tabby (Brown / Chocolate)  color_primary_Tabby (Buff / Tan / Fawn)  color_primary_Tabby (Gray / Blue / Silver)  color_primary_Tabby (Orange / Red)  color_primary_Tabby (Tiger Striped)  color_primary_Torbie  color_primary_Tortoiseshell  color_primary_White  color_primary_unknown  gender_Female  goodwith_cats_True  goodwith_cats_unknown  goodwith_children_True  goodwith_children_unknown  goodwith_dogs_True  goodwith_dogs_unknown  size_Large  size_Medium  size_Small\n",
       "count   6489.00000 6489.00000 6489.00000 6489.00000 6489.00000     6489.00000  6489.00000     6489.00000       6489.00000     6489.00000 6489.00000 6489.00000 6489.00000                6489.00000                        6489.00000              6489.00000            6489.00000            6489.00000            6489.00000                       6489.00000             6489.00000            6489.00000               6489.00000               6489.00000                   6489.00000                          6489.00000                        6489.00000                          6489.00000                         6489.00000                  6489.00000                      6489.00000                                           6489.00000               6489.00000                6489.00000          6489.00000              6489.00000                          6489.00000             6489.00000              6489.00000                6489.00000             6489.00000                  6489.00000                   6489.00000             6489.00000            6489.00000               6489.00000              6489.00000           6489.00000           6489.00000               6489.00000            6489.00000                   6489.00000                    6489.00000                 6489.00000            6489.00000   6489.00000              6489.00000          6489.00000       6489.00000    6489.00000      6489.00000       6489.00000     6489.00000     6489.00000       6489.00000       6489.00000    6489.00000        6489.00000     6489.00000          6489.00000  6489.00000      6489.00000     6489.00000      6489.00000    6489.00000       6489.00000     6489.00000         6489.00000    6489.00000     6489.00000         6489.00000    6489.00000    6489.00000  6489.00000         6489.00000          6489.00000    6489.00000     6489.00000     6489.00000   6489.00000    6489.00000    6489.00000     6489.00000   6489.00000    6489.00000   6489.00000    6489.00000           6489.00000     6489.00000       6489.00000   6489.00000   6489.00000   6489.00000      6489.00000      6489.00000          6489.00000        6489.00000   6489.00000 6489.00000   6489.00000  6489.00000    6489.00000           6489.00000                            6489.00000                6489.00000                6489.00000                       6489.00000                  6489.00000                       6489.00000            6489.00000                     6489.00000                   6489.00000                 6489.00000                   6489.00000                          6489.00000                 6489.00000                  6489.00000                          6489.00000                 6489.00000                    6489.00000                  6489.00000                6489.00000           6489.00000                               6489.00000                               6489.00000                                  6489.00000                          6489.00000                           6489.00000            6489.00000                   6489.00000           6489.00000             6489.00000     6489.00000          6489.00000             6489.00000              6489.00000                 6489.00000          6489.00000             6489.00000  6489.00000   6489.00000  6489.00000\n",
       "mean      -0.00000    0.00000   -0.00000   -0.00000   -0.00000        0.00000    -0.00000        0.00000         -0.00000       -0.00000    0.00000    0.00000    0.00000                  -0.00000                           0.00000                -0.00000               0.00000              -0.00000               0.00000                         -0.00000                0.00000               0.00000                 -0.00000                 -0.00000                     -0.00000                            -0.00000                          -0.00000                            -0.00000                            0.00000                     0.00000                        -0.00000                                              0.00000                  0.00000                  -0.00000            -0.00000                 0.00000                            -0.00000               -0.00000                -0.00000                  -0.00000               -0.00000                    -0.00000                     -0.00000               -0.00000               0.00000                  0.00000                 0.00000             -0.00000             -0.00000                 -0.00000              -0.00000                     -0.00000                      -0.00000                   -0.00000              -0.00000     -0.00000                -0.00000             0.00000          0.00000       0.00000         0.00000         -0.00000       -0.00000       -0.00000         -0.00000         -0.00000       0.00000           0.00000       -0.00000            -0.00000    -0.00000         0.00000       -0.00000        -0.00000      -0.00000          0.00000       -0.00000           -0.00000       0.00000        0.00000           -0.00000      -0.00000      -0.00000     0.00000           -0.00000            -0.00000       0.00000       -0.00000        0.00000      0.00000      -0.00000      -0.00000        0.00000      0.00000       0.00000      0.00000       0.00000             -0.00000        0.00000         -0.00000     -0.00000     -0.00000     -0.00000         0.00000         0.00000             0.00000          -0.00000      0.00000    0.00000     -0.00000    -0.00000       0.00000              0.00000                               0.00000                  -0.00000                   0.00000                          0.00000                    -0.00000                         -0.00000               0.00000                       -0.00000                      0.00000                    0.00000                      0.00000                            -0.00000                   -0.00000                    -0.00000                            -0.00000                    0.00000                      -0.00000                    -0.00000                   0.00000             -0.00000                                 -0.00000                                 -0.00000                                    -0.00000                             0.00000                              0.00000               0.00000                      0.00000              0.00000                0.00000        0.00000             0.00000               -0.00000                -0.00000                   -0.00000            -0.00000                0.00000    -0.00000      0.00000     0.00000\n",
       "std        1.00008    1.00008    1.00008    1.00008    1.00008        1.00008     1.00008        1.00008          1.00008        1.00008    1.00008    1.00008    1.00008                   1.00008                           1.00008                 1.00008               1.00008               1.00008               1.00008                          1.00008                1.00008               1.00008                  1.00008                  1.00008                      1.00008                             1.00008                           1.00008                             1.00008                            1.00008                     1.00008                         1.00008                                              1.00008                  1.00008                   1.00008             1.00008                 1.00008                             1.00008                1.00008                 1.00008                   1.00008                1.00008                     1.00008                      1.00008                1.00008               1.00008                  1.00008                 1.00008              1.00008              1.00008                  1.00008               1.00008                      1.00008                       1.00008                    1.00008               1.00008      1.00008                 1.00008             1.00008          1.00008       1.00008         1.00008          1.00008        1.00008        1.00008          1.00008          1.00008       1.00008           1.00008        1.00008             1.00008     1.00008         1.00008        1.00008         1.00008       1.00008          1.00008        1.00008            1.00008       1.00008        1.00008            1.00008       1.00008       1.00008     1.00008            1.00008             1.00008       1.00008        1.00008        1.00008      1.00008       1.00008       1.00008        1.00008      1.00008       1.00008      1.00008       1.00008              1.00008        1.00008          1.00008      1.00008      1.00008      1.00008         1.00008         1.00008             1.00008           1.00008      1.00008    1.00008      1.00008     1.00008       1.00008              1.00008                               1.00008                   1.00008                   1.00008                          1.00008                     1.00008                          1.00008               1.00008                        1.00008                      1.00008                    1.00008                      1.00008                             1.00008                    1.00008                     1.00008                             1.00008                    1.00008                       1.00008                     1.00008                   1.00008              1.00008                                  1.00008                                  1.00008                                     1.00008                             1.00008                              1.00008               1.00008                      1.00008              1.00008                1.00008        1.00008             1.00008                1.00008                 1.00008                    1.00008             1.00008                1.00008     1.00008      1.00008     1.00008\n",
       "min       -1.52233   -0.09497   -1.49233   -5.38903   -0.19213       -1.26894    -0.52865       -4.48741         -2.86887       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793    -1.76561      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454     -1.90570    -0.38825\n",
       "25%       -1.52233   -0.09497   -0.75585    0.18556   -0.19213       -1.26894    -0.42834        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                           -1.24729                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021       -1.04461            -0.98213               -0.92987                -0.60340                   -1.36886            -0.49136               -1.71113    -0.29454      0.52474    -0.38825\n",
       "50%        0.65689   -0.09497   -0.30815    0.18556   -0.19213        0.78806    -0.25138        0.22285          0.34857       -0.13900   -0.71374   -0.91726   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730            -0.98213               -0.92987                -0.60340                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "75%        0.65689   -0.09497    1.00801    0.18556   -0.19213        0.78806     0.23704        0.22285          0.34857       -0.13900    1.40108    1.09020   -0.44436                  -0.01756                          -0.06093                -0.01756              -0.02484              -0.02151              -0.03513                         -0.01756               -0.01241              -0.12058                 -0.01756                 -0.02484                     -0.06219                            -0.04972                          -0.26871                            -0.34270                            0.80174                    -0.01241                        -0.02484                                             -0.04480                 -0.03513                  -0.06928            -0.04650                -0.01241                            -0.01756               -0.06700                -0.02151                  -0.01756               -0.04304                    -0.05698                     -0.03042               -0.18288              -0.01241                 -0.01241                -0.05560             -0.32007             -0.02151                 -0.01756              -0.06343                     -0.10367                      -0.01756                   -0.01756              -0.15225     -0.02777                -0.03042            -0.03042         -0.22850      -0.14354        -0.13842         -0.13313       -0.07040       -0.02151         -0.08071         -0.04972      -0.09414          -0.17178       -0.08723            -0.07776    -0.08633        -0.05833       -0.18198        -0.02484      -0.12887         -0.15487       -0.36774           -0.13785      -0.04650       -0.06815           -0.16939      -0.06700      -0.04121    -0.23295           -0.18821            -0.12763      -0.03286       -0.23949       -0.05419     -0.01241      -0.01756      -0.10518       -0.15591     -0.03513      -0.13192     -0.10137      -0.09075             -0.14185       -0.31802         -0.03042     -0.06700     -0.03042     -0.03286        -0.01241        -0.01241            -0.03513          -0.45278     -0.20836   -0.30067     -0.37793     0.56638      -0.18909             -0.53381                              -0.34968                  -0.05560                  -0.06464                         -0.18378                    -0.08357                         -0.10367              -0.17462                       -0.05560                     -0.09497                   -0.05125                     -0.09246                            -0.10367                   -0.07876                    -0.24766                            -0.23769                   -0.06583                      -0.18952                    -0.15900                  -0.10886             -0.05125                                 -0.36205                                 -0.12948                                    -0.24941                            -0.21117                             -0.10059              -0.12447                     -0.16987             -0.10367               -0.24021        0.95730             1.01820                1.07542                 1.65727                    0.73053            -0.49136                0.58441    -0.29454      0.52474    -0.38825\n",
       "max        0.65689   10.52992    2.26947    0.18556    5.20489        0.78806     6.93423        0.22285          0.34857        7.19417    1.40108    1.09020    2.25044                  56.95173                          16.41265                56.95173              40.26475              46.49731              28.46269                         56.95173               80.54812               8.29302                 56.95173                 40.26475                     16.07980                            20.11374                           3.72142                             2.91799                            0.80174                    80.54812                        40.26475                                             22.31936                 28.46269                  14.43338            21.50581                80.54812                            56.95173               14.92510                46.49731                  56.95173               23.23252                    17.54993                     32.87096                5.46809              80.54812                 80.54812                17.98472              3.12429             46.49731                 56.95173              15.76632                      9.64590                      56.95173                   56.95173               6.56832     36.01111                32.87096            32.87096          4.37632       6.96666         7.22416          7.51164       14.20497       46.49731         12.38951         20.11374      10.62272           5.82126       11.46423            12.86019    11.58393        17.14510        5.49519        40.26475       7.75996          6.45684        2.71930            7.25452      21.50581       14.67310            5.90346      14.92510      24.26745     4.29280            5.31317             7.83545      30.43025        4.17548       18.45336     80.54812      56.95173       9.50760        6.41376     28.46269       7.58020      9.86500      11.01971              7.04949        3.14445         32.87096     14.92510     32.87096     30.43025        80.54812        80.54812            28.46269           2.20856      4.79931    3.32586      2.64598     0.56638       5.28855              1.87333                               2.85976                  17.98472                  15.47040                          5.44137                    11.96662                          9.64590               5.72686                       17.98472                     10.52992                   19.51169                     10.81581                             9.64590                   12.69744                     4.03782                             4.20717                   15.19046                       5.27636                     6.28938                   9.18594             19.51169                                  2.76206                                  7.72301                                     4.00953                             4.73561                              9.94137               8.03402                      5.88675              9.64590                4.16299        0.95730             1.01820                1.07542                 1.65727                    0.73053             2.03515                0.58441     3.39515      0.52474     2.57568"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X = pd.DataFrame(scaled, columns=X.columns)\n",
    "scaled_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.3, random_state=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542, 152), (1947, 152))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4542,), (1947,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_X_test.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_train.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)y\n",
      "Writing file.  \"data/tt_sets/cats_y_test.csv\"\n"
     ]
    }
   ],
   "source": [
    "# save training and test sets\n",
    "datapath = 'data/tt_sets'\n",
    "save_file(X_train, 'cats_X_train.csv', datapath)\n",
    "save_file(X_test, 'cats_X_test.csv', datapath)\n",
    "save_file(y_train, 'cats_y_train.csv', datapath)\n",
    "save_file(y_test, 'cats_y_test.csv', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "### Goal: Build two to three different models and identify the best one.\n",
    "<ul><li>Fit your models with a training dataset</li>\n",
    "<li>Review model outcomes â Iterate over additional models as needed</li>\n",
    "<li>Identify the final model that you think is the best model for this project</li></ul>\n",
    " Review the following questions and apply them to your analysis: \n",
    "<ul><li>Does my data involve a time series or forecasting? If so, am I splitting the train and test data appropriately?</li>\n",
    "<li>Is my response variable continuous or categorical?</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7832782367335216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10267818, 0.06431094, 0.11979787, 0.07698541, 0.14252197])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_scores_preopt = rf_cv['test_score']\n",
    "rf_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10125887423677346, 0.02829382379509712)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_cv_scores_preopt), np.std(rf_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  34.435340\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)\n",
    "rmse_rf_preopt = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "print(\"RMSE : % f\" %(rmse_rf_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "rf_grid_params = {\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3,4,5, 6,7,8,9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'n_estimators': 297}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gcv = GridSearchCV(rf, param_grid=rf_grid_params, cv=5, n_jobs=-1)\n",
    "gcv.fit(X_train, y_train)\n",
    "gcv_params = gcv.best_params_\n",
    "\n",
    "gcv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4145858789830231"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=gcv_params['n_estimators'], max_depth=gcv_params['max_depth'])\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16474897, 0.15439094, 0.17696468, 0.15925713, 0.19595176]),\n",
       " array([0.10877302, 0.12327886, 0.0328471 , 0.07556306, 0.16593833]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_train = cross_validate(rf, X_train, y_train, cv=5)\n",
    "rf_cv_test = cross_validate(rf, X_test, y_test, cv=5)\n",
    "rf_cv_train['test_score'], rf_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Score, Training Set: 0.17026269550813455\n",
      "Average CV Score, Trest Set: 0.10128007258635516\n"
     ]
    }
   ],
   "source": [
    "rf_train_score = np.mean(rf_cv_train['test_score'])\n",
    "rf_test_score = np.mean(rf_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", rf_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", rf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Training Set :  27.596311\n",
      "RMSE Test Set :  32.374979\n"
     ]
    }
   ],
   "source": [
    "rf_train_pred = rf.predict(X_train)\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "rf_rmse_train = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_rmse_test = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(rf_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(rf_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31787013710575396"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18857688, 0.18959194, 0.176784  , 0.18527591, 0.18640978])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_scores_preopt = gb_cv['test_score']\n",
    "gb_cv_scores_preopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18532770274172977, 0.004536720917895641)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gb_cv_scores_preopt), np.std(gb_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  32.265180\n"
     ]
    }
   ],
   "source": [
    "gb_pred = gb.predict(X_test)\n",
    "rmse_gb_preopt = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "print(\"RMSE : % f\" %(rmse_gb_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "gb_grid_params = {\n",
    "        'learning_rate': [.01, .1, 1],\n",
    "        'n_estimators': n_est,\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 1],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n",
       "                         'n_estimators': [10, 12, 16, 20, 26, 33, 42, 54, 69,\n",
       "                                          88, 112, 143, 183, 233, 297, 379, 483,\n",
       "                                          615, 784, 1000]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_cv = GridSearchCV(gb, param_grid=gb_grid_params, cv=5, n_jobs=-1)\n",
    "gb_grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid_cv.fit(X_train, y_train)\n",
    "gb_grid_cv_params = gb_grid_cv.best_params_\n",
    "\n",
    "gb_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=gb_grid_cv_params['n_estimators'], max_depth=gb_grid_cv_params['max_depth'], learning_rate=gb_grid_cv_params['learning_rate'])\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cv_train = cross_validate(gb, X_train, y_train, cv=5)\n",
    "gb_cv_test = cross_validate(gb, X_test, y_test, cv=5)\n",
    "gb_cv_train['test_score'], gb_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_train_score = np.mean(gb_cv_train['test_score'])\n",
    "gb_test_score = np.mean(gb_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", gb_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", gb_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_train_pred = gb.predict(X_train)\n",
    "gb_test_pred = gb.predict(X_test)\n",
    "gb_rmse_train = np.sqrt(mean_squared_error(y_train, gb_train_pred))\n",
    "gb_rmse_test = np.sqrt(mean_squared_error(y_test, gb_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(gb_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(gb_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=25, weights='distance')\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_cv = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_scores_preopt = kn_cv['test_score']\n",
    "np.mean(kn_cv_scores_preopt), np.std(kn_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_pred = kn.predict(X_test)\n",
    "rmse_kn_preopt = np.sqrt(mean_squared_error(y_test, kn_pred))\n",
    "print(\"RMSE : % f\" %(rmse_kn_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "kn_grid_params = {\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'n_neighbors': n_est,\n",
    "        'p': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_grid_cv = GridSearchCV(kn, param_grid=kn_grid_params, cv=5, n_jobs=-1)\n",
    "kn_grid_cv.fit(X_train, y_train)\n",
    "kn_grid_cv_params = kn_grid_cv.best_params_\n",
    "\n",
    "kn_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KNeighborsRegressor(n_neighbors=kn_grid_cv_params['n_neighbors'], weights=kn_grid_cv_params['weights'], p=kn_grid_cv_params['p'])\n",
    "kn.fit(X_train, y_train)\n",
    "kn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_cv_train = cross_validate(kn, X_train, y_train, cv=5)\n",
    "kn_cv_test = cross_validate(kn, X_test, y_test, cv=5)\n",
    "kn_cv_train['test_score'], kn_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_train_score = np.mean(kn_cv_train['test_score'])\n",
    "kn_test_score = np.mean(kn_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", kn_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", kn_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_train_pred = kn.predict(X_train)\n",
    "kn_test_pred = kn.predict(X_test)\n",
    "kn_rmse_train = np.sqrt(mean_squared_error(y_train, kn_train_pred))\n",
    "kn_rmse_test = np.sqrt(mean_squared_error(y_test, kn_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(kn_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(kn_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 50)\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cv = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_scores_preopt = xg_cv['test_score']\n",
    "np.mean(xg_cv_scores_preopt), np.std(xg_cv_scores_preopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred = xg.predict(X_test)\n",
    "rmse_xg_preopt = np.sqrt(mean_squared_error(y_test, xg_pred))\n",
    "print(\"RMSE : % f\" %(rmse_xg_preopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
    "xg_grid_params = {\n",
    "        'objective': ['reg:squarederror', 'reg:squaredlogerror', 'reg:logistic'],\n",
    "        'n_estimators': n_est,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_grid_cv = GridSearchCV(xg, param_grid=xg_grid_params, cv=5, n_jobs=-1)\n",
    "xg_grid_cv.fit(X_train, y_train)\n",
    "xg_grid_cv_params = xg_grid_cv.best_params_\n",
    "\n",
    "xg_grid_cv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBRegressor(objective=xg_grid_cv_params['objective'], n_estimators = xg_grid_cv_params['n_estimators'])\n",
    "xg.fit(X_train, y_train)\n",
    "xg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cv_train = cross_validate(xg, X_train, y_train, cv=5)\n",
    "xg_cv_test = cross_validate(xg, X_test, y_test, cv=5)\n",
    "xg_cv_train['test_score'], xg_cv_test['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train_score = np.mean(xg_cv_train['test_score'])\n",
    "xg_test_score = np.mean(xg_cv_test['test_score'])\n",
    "\n",
    "print(\"Average CV Score, Training Set:\", xg_train_score)\n",
    "print(\"Average CV Score, Trest Set:\", xg_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train_pred = xg.predict(X_train)\n",
    "xg_test_pred = xg.predict(X_test)\n",
    "xg_rmse_train = np.sqrt(mean_squared_error(y_train, xg_train_pred))\n",
    "xg_rmse_test = np.sqrt(mean_squared_error(y_test, xg_test_pred))\n",
    "print(\"RMSE Training Set : % f\" %(xg_rmse_train))\n",
    "print(\"RMSE Test Set : % f\" %(xg_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = pd.DataFrame({'CV Train': [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score)], 'RMSE Train': [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train],'CV Test': [np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)], 'RMSE Test': [rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]}, index=['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost'])\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with best CV Score: \\nTrain:\", model_scores['CV Train'].idxmin(), \"\\nTest:\", model_scores['CV Test'].idxmin())\n",
    "print(\"\\nModel with best RMSE: \\nTrain:\", model_scores['RMSE Train'].idxmin(), \"\\nTest:\", model_scores['RMSE Test'].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost','RandomForest', 'GradientBoosting', 'KNNeighbors', 'XGBoost']\n",
    "cv_scores_all = [np.mean(rf_train_score), np.mean(gb_train_score), np.mean(kn_train_score), np.mean(xg_train_score), np.mean(rf_test_score), np.mean(gb_test_score), np.mean(kn_test_score), np.mean(xg_test_score)]\n",
    "types = ['train', 'train', 'train', 'train', 'test', 'test', 'test', 'test']\n",
    "rmse_scores_all = [rf_rmse_train, gb_rmse_train, kn_rmse_train, xg_rmse_train, rf_rmse_test, gb_rmse_test, kn_rmse_test, xg_rmse_test]\n",
    "\n",
    "cv_scores = pd.DataFrame(list(zip(models, cv_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) \n",
    "rmse_scores = pd.DataFrame(list(zip(models, rmse_scores_all, types)), \n",
    "               columns =['Model', 'Scores', 'Type' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=cv_scores)\n",
    "plt.title(\"CV Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x='Model', y='Scores', hue='Type', data=rmse_scores)\n",
    "plt.title(\"RMSE Score Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go forward with **GradientBoosting** for the cats data. I'm choosing this model because it has the best RMSE score for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_test = np.std(y_test)\n",
    "std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_test = model_scores.drop(columns=['CV Train','RMSE Train']).rename(columns={'CV Test':'R^2', 'RMSE Test':'RMSE'})\n",
    "\n",
    "#standardize the RMSE by dividing it by the standard deviation of y_test\n",
    "model_scores_test['RMSE'] = model_scores_test['RMSE']/std_test\n",
    "\n",
    "#sort scores by lowest RMSE\n",
    "model_scores_test.sort_values(by='RMSE', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=np.round(model_scores_test.values,4), \n",
    "    colLabels=model_scores_test.columns, \n",
    "    rowLabels=model_scores_test.index, \n",
    "    loc='center', \n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.scale(.75, 3)\n",
    "\n",
    "plt.title('Model Metrics: Cats', size=16)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('assets/cats_model_metrics.png', bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
